{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import natsort\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from lib import check_padlocks\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "blast_file_dir = Path(\"/nemo/lab/znamenskiyp/scratch/olfrs_monahan/blast_queries/\")\n",
    "check_padlocks.loaddb(\"mouse\", config)\n",
    "\n",
    "genes = [\n",
    "    os.path.basename(file).split(\"_query\")[0]\n",
    "    for file in os.listdir(blast_file_dir)\n",
    "    if file.endswith(\"query_blast.out\")\n",
    "]\n",
    "print(genes)\n",
    "print(len(genes))\n",
    "failed_matches = []\n",
    "for gene in genes:\n",
    "    failed_match = check_padlocks.find_off_targets(gene, blast_file_dir)\n",
    "    if failed_match:\n",
    "        failed_matches.append(failed_match)\n",
    "\n",
    "# make a df from all the files that end with _off_targets.out\n",
    "files = glob.glob(str(blast_file_dir / \"*_off_targets.out\"))\n",
    "# ignore the first row of each file\n",
    "df = pd.concat([pd.read_csv(file, header=0) for file in files])\n",
    "# sort df with natsort on column\n",
    "df = df.iloc[natsort.index_natsorted(df[\"0\"])]\n",
    "header = [\n",
    "    \"query\",\n",
    "    \"subject\",\n",
    "    \"percentage identity\",\n",
    "    \"length\",\n",
    "    \"mismatches\",\n",
    "    \"gaps\",\n",
    "    \"qstart\",\n",
    "    \"qend\",\n",
    "    \"sstart\",\n",
    "    \"send\",\n",
    "    \"evalue\",\n",
    "    \"bitscore\",\n",
    "    \"qseq\",\n",
    "    \"sseq\",\n",
    "    \"homology_candidate_hit\",\n",
    "    \"gene\",\n",
    "]\n",
    "# remove the first column\n",
    "df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "df.columns = header\n",
    "df.sort_values(by=[\"evalue\"], inplace=True)\n",
    "\n",
    "precomputed_variants = check_padlocks.precompute_variants(df)\n",
    "\n",
    "# Get unique variants to reduce the number of lookups\n",
    "unique_variants = df[\"subject\"].unique()\n",
    "\n",
    "# Get genes for unique variants\n",
    "genes_dict = check_padlocks.find_genes_from_variants(unique_variants, \"mouse\", config)\n",
    "\n",
    "# Map genes back to the DataFrame\n",
    "df[\"blast_target\"] = df[\"subject\"].map(genes_dict)\n",
    "\n",
    "# Simplify the 'offtarget' field\n",
    "df[\"blast_target\"] = df[\"blast_target\"].apply(\n",
    "    lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None\n",
    ")\n",
    "\n",
    "# Running the optimized processing function\n",
    "df = check_padlocks.process_dataframe(\n",
    "    df, armlength=20, tm_threshold=37, precomputed_variants=precomputed_variants\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def create_missing_fasta_list(fasta_list_path):\n",
    "    fasta_list_path = Path(fasta_list_path).expanduser().resolve()\n",
    "\n",
    "    # Read existing fasta entries\n",
    "    with open(fasta_list_path) as f:\n",
    "        fasta_paths = [Path(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "    if not fasta_paths:\n",
    "        raise ValueError(\"fasta_list.txt is empty.\")\n",
    "\n",
    "    folder = fasta_paths[0].parent\n",
    "\n",
    "    # Genes listed in fasta_list.txt\n",
    "    genes_in_fasta = set()\n",
    "    for p in fasta_paths:\n",
    "        if p.name.endswith(\"_query.fasta\"):\n",
    "            genes_in_fasta.add(p.name.rsplit(\"_query.fasta\", 1)[0])\n",
    "\n",
    "    # Genes that already have a _query_blast.out file\n",
    "    blast_files = folder.glob(\"*_query_blast.out\")\n",
    "    genes_with_blast = {bf.name.rsplit(\"_query_blast.out\", 1)[0] for bf in blast_files}\n",
    "\n",
    "    # Genes missing BLAST output\n",
    "    missing_genes = sorted(genes_in_fasta - genes_with_blast)\n",
    "\n",
    "    out_path = folder / \"fasta_list2.txt\"\n",
    "    with open(out_path, \"w\") as out:\n",
    "        for gene in missing_genes:\n",
    "            out.write(str(folder / f\"{gene}_query.fasta\") + \"\\n\")\n",
    "\n",
    "    print(f\"Total genes in fasta_list: {len(genes_in_fasta)}\")\n",
    "    print(f\"Genes with BLAST output: {len(genes_with_blast & genes_in_fasta)}\")\n",
    "    print(f\"Missing BLAST outputs:   {len(missing_genes)}\")\n",
    "    print(f\"Wrote: {out_path}\")\n",
    "\n",
    "\n",
    "# Run (update the path if needed)\n",
    "create_missing_fasta_list(\n",
    "    \"/nemo/lab/znamenskiyp/scratch/olfrs_monahan/blast_queries/fasta_list.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Olfrs_monahan_BLAST_Tm_results_split_arms.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Melting temp (requires Melting 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melting_5 = True\n",
    "if melting_5:\n",
    "    df = check_padlocks.process_dataframe_in_batches(df, batch_size=300)\n",
    "    df.to_csv(\"Olfrs_monahan_BLAST_Tm_results_split_arms.csv\", index=False)\n",
    "else:\n",
    "    df = pd.read_csv(\"blast_results_olfr_full_across_tms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Tm vs no. padlocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the True values for each \"valid_xx\" column\n",
    "valid_columns = [f\"valid_{i}\" for i in range(20, 51)]\n",
    "true_sums = df[valid_columns].sum()\n",
    "\n",
    "# Plotting the sums\n",
    "plt.figure(figsize=(8, 5), dpi=200)\n",
    "plt.plot(valid_columns, true_sums, marker=\"o\")\n",
    "\n",
    "plt.xlabel(\"Melting5 Tm\")\n",
    "plt.ylabel(\"Number of valid padlocks\")\n",
    "# plt.grid(True)\n",
    "plt.xticks(range(len(valid_columns)), range(20, 51))\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot number of valid padlocks per gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_queries = df.groupby(\"query\")\n",
    "\n",
    "df_padlocks = grouped_queries.agg(\n",
    "    valid_specific=(\n",
    "        \"query\",\n",
    "        lambda x: any(df.loc[x.index, \"valid_probe_NN\"] & df.loc[x.index, \"specific\"]),\n",
    "    ),\n",
    "    valid_non_specific=(\n",
    "        \"query\",\n",
    "        lambda x: any(df.loc[x.index, \"valid_probe_NN\"] & ~df.loc[x.index, \"specific\"]),\n",
    "    ),\n",
    ").reset_index()\n",
    "\n",
    "# Merge df_padlocks with the original df to bring padlock_target_gene into df_padlocks\n",
    "df_merged = pd.merge(\n",
    "    df_padlocks, df[[\"query\", \"gene\"]], on=\"query\", how=\"left\"\n",
    ").drop_duplicates()\n",
    "\n",
    "# Group by gene and aggregate valid_specific and valid_non_specific\n",
    "df_grouped_by_gene = (\n",
    "    df_merged.groupby(\"gene\")\n",
    "    .agg(\n",
    "        valid_specific=(\"valid_specific\", \"any\"),\n",
    "        valid_non_specific=(\"valid_non_specific\", \"any\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Count the number of padlocks\n",
    "df_counts = (\n",
    "    df_merged.groupby(\"gene\")\n",
    "    .agg(\n",
    "        number_of_specific_padlocks=(\"valid_specific\", lambda x: x.sum()),\n",
    "        number_of_non_specific_padlocks=(\"valid_non_specific\", lambda x: x.sum()),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_padlocks.to_csv(\"padlock_specificity_olfrs.csv\", index=False)\n",
    "df_grouped_by_gene.to_csv(\"gene_specificity_olfrs.csv\", index=False)\n",
    "df_counts.to_csv(\"number_of_specific_padlocks_olfrs.csv\", index=False)\n",
    "\n",
    "# Using df_counts to get the counts of specific and non-specific padlocks\n",
    "padlocks_per_gene_specific = df_counts.set_index(\"gene\")[\"number_of_specific_padlocks\"]\n",
    "padlocks_per_gene_non_specific = df_counts.set_index(\"gene\")[\n",
    "    \"number_of_non_specific_padlocks\"\n",
    "]\n",
    "\n",
    "# Combine the counts into one DataFrame for plotting\n",
    "genes = padlocks_per_gene_specific.index.union(padlocks_per_gene_non_specific.index)\n",
    "combined_counts = pd.DataFrame(\n",
    "    {\n",
    "        \"Specific Padlocks\": padlocks_per_gene_specific.reindex(genes, fill_value=0),\n",
    "        \"Non-Specific Padlocks\": padlocks_per_gene_non_specific.reindex(\n",
    "            genes, fill_value=0\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Plotting the data\n",
    "fig, ax = plt.subplots(figsize=(60, 6))\n",
    "combined_counts[\"Specific Padlocks\"].plot(\n",
    "    kind=\"bar\", color=\"black\", ax=ax, position=0, width=0.4\n",
    ")\n",
    "combined_counts[\"Non-Specific Padlocks\"].plot(\n",
    "    kind=\"bar\", color=\"red\", ax=ax, position=1, width=0.4\n",
    ")\n",
    "\n",
    "# Customizing the plot\n",
    "ax.set_title(\"Specific and Non-Specific Padlocks per Gene\")\n",
    "ax.set_xlabel(\"Gene\")\n",
    "ax.set_ylabel(\"Number of Padlocks\")\n",
    "ax.legend([\"Specific Padlocks\", \"Non-Specific Padlocks\"])\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Tm of the left and right arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=200)\n",
    "plt.title(\"Including ligation site missmatch seqs\")\n",
    "\n",
    "# Scatter: unchanged\n",
    "sc = plt.scatter(\n",
    "    df[\"tm_right_NN\"],\n",
    "    df[\"tm_left_NN\"],  # df[\"tm_right_melting\"],\n",
    "    c=df[\"mismatches\"],\n",
    "    s=0.1,\n",
    "    vmin=0,\n",
    "    vmax=6,\n",
    ")\n",
    "\n",
    "kde_n = 100000\n",
    "df_kde = df[[\"tm_right_NN\", \"tm_left_NN\"]].dropna()\n",
    "if len(df_kde) > kde_n:\n",
    "    df_kde = df_kde.sample(n=kde_n, random_state=42)\n",
    "\n",
    "sns.kdeplot(\n",
    "    x=df_kde[\"tm_right_NN\"],\n",
    "    y=df_kde[\"tm_left_NN\"],\n",
    "    levels=10,\n",
    "    gridsize=120,  # lower = faster; default is 200\n",
    "    cut=0,  # don't evaluate beyond data range\n",
    "    bw_adjust=0.4,\n",
    "    palette=\"Reds\",\n",
    ")\n",
    "\n",
    "# Colorbar tied to the scatter (so it reflects mismatches)\n",
    "cbar = plt.colorbar(sc, label=\"No. total mismatches/gaps\", fraction=0.046, pad=0.04)\n",
    "\n",
    "# Line y=x\n",
    "plt.plot([-50, 100], [-50, 100], color=\"red\", lw=1, alpha=0.3)\n",
    "\n",
    "plt.xlabel(\"Tm_NN right (C)\")\n",
    "plt.ylabel(\"Tm_NN left (C)\")\n",
    "plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "plt.xticks(np.arange(-50, 110, 10))\n",
    "plt.yticks(np.arange(-50, 110, 10))\n",
    "plt.axvline(0, color=\"black\", lw=0.5)\n",
    "plt.axhline(0, color=\"black\", lw=0.5)\n",
    "plt.ylim(-50, 80)\n",
    "plt.xlim(-50, 70)\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", lw=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.figure(figsize=(8, 8), dpi=200)\n",
    "plt.title(\"NN probe cutoff\")\n",
    "\n",
    "# Plot where valid_probe_NN is False (black)\n",
    "valid_false = df[df[\"valid_probe_NN\"] == False]\n",
    "plt.scatter(\n",
    "    valid_false[\"tm_left_NN\"],\n",
    "    valid_false[\"tm_left_melting\"],\n",
    "    c=\"black\",\n",
    "    s=0.1,\n",
    "    label=\"Valid Probe NN (False)\",\n",
    ")\n",
    "plt.scatter(\n",
    "    valid_false[\"tm_right_NN\"], valid_false[\"tm_right_melting\"], c=\"black\", s=0.1\n",
    ")\n",
    "\n",
    "# Plot where valid_probe_NN is True (green)\n",
    "valid_true = df[df[\"valid_probe_NN\"] == True]\n",
    "plt.scatter(\n",
    "    valid_true[\"tm_left_NN\"],\n",
    "    valid_true[\"tm_left_melting\"],\n",
    "    c=\"lime\",\n",
    "    s=2,\n",
    "    label=\"Valid Probe NN (True)\",\n",
    ")\n",
    "plt.scatter(\n",
    "    valid_true[\"tm_right_NN\"],\n",
    "    valid_true[\"tm_right_melting\"],\n",
    "    c=\"lime\",\n",
    "    s=2,\n",
    ")\n",
    "\n",
    "# Plot the line y=x\n",
    "plt.plot([-50, 100], [-50, 100], color=\"red\", lw=1, alpha=0.3)\n",
    "\n",
    "# Add red vertical line at 37 degrees\n",
    "plt.axvline(37, color=\"red\", lw=0.5)\n",
    "\n",
    "\n",
    "# Set labels and limits\n",
    "plt.xlabel(\"Tm_NN (C)\")\n",
    "plt.ylabel(\"Tm Melting5 (C)\")\n",
    "plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "# Add grid lines every 10 degrees\n",
    "plt.xticks(np.arange(-50, 110, 10))\n",
    "plt.yticks(np.arange(-50, 110, 10))\n",
    "\n",
    "# Add bold line at 0\n",
    "plt.axvline(0, color=\"black\", lw=0.5)\n",
    "plt.axhline(0, color=\"black\", lw=0.5)\n",
    "\n",
    "# Set plot limits\n",
    "plt.ylim(-50, 80)\n",
    "plt.xlim(-50, 70)\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", lw=0.5)\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.figure(figsize=(8, 8), dpi=200)\n",
    "plt.title(\"Melting5 probe cutoff\")\n",
    "\n",
    "# Plot where valid_probe_NN is False (black)\n",
    "valid_false = df[df[\"valid_probe_melting\"] == False]\n",
    "plt.scatter(\n",
    "    valid_false[\"tm_left_NN\"],\n",
    "    valid_false[\"tm_left_melting\"],\n",
    "    c=\"black\",\n",
    "    s=0.1,\n",
    "    label=\"Valid Probe Melting5 (False)\",\n",
    ")\n",
    "plt.scatter(\n",
    "    valid_false[\"tm_right_NN\"], valid_false[\"tm_right_melting\"], c=\"black\", s=0.1\n",
    ")\n",
    "\n",
    "# Plot where valid_probe_NN is True (green)\n",
    "valid_true = df[df[\"valid_probe_melting\"] == True]\n",
    "plt.scatter(\n",
    "    valid_true[\"tm_left_NN\"],\n",
    "    valid_true[\"tm_left_melting\"],\n",
    "    c=\"lime\",\n",
    "    s=2,\n",
    "    label=\"Valid Probe Melting5 (True)\",\n",
    ")\n",
    "plt.scatter(valid_true[\"tm_right_NN\"], valid_true[\"tm_right_melting\"], c=\"lime\", s=2)\n",
    "\n",
    "# Add red line horizontal at 37\n",
    "plt.axhline(37, color=\"red\", lw=0.5)\n",
    "\n",
    "# Plot the line y=x\n",
    "plt.plot([-50, 100], [-50, 100], color=\"red\", lw=1, alpha=0.3)\n",
    "\n",
    "# Set labels and limits\n",
    "plt.xlabel(\"Tm_NN (C)\")\n",
    "plt.ylabel(\"Tm Melting5 (C)\")\n",
    "plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "# Add grid lines every 10 degrees\n",
    "plt.xticks(np.arange(-50, 110, 10))\n",
    "plt.yticks(np.arange(-50, 110, 10))\n",
    "\n",
    "# Add bold line at 0\n",
    "plt.axvline(0, color=\"black\", lw=0.5)\n",
    "plt.axhline(0, color=\"black\", lw=0.5)\n",
    "\n",
    "# Set plot limits\n",
    "plt.ylim(-50, 80)\n",
    "plt.xlim(-50, 70)\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", lw=0.5)\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn3\n",
    "from matplotlib_venn.layout.venn3 import DefaultLayoutAlgorithm\n",
    "\n",
    "# Create sets for each condition\n",
    "set_NN = set(df.index[df[\"valid_probe_NN\"]])\n",
    "set_melting = set(df.index[df[\"valid_probe_melting\"]])\n",
    "set_old_filters = set(df.index[df[\"valid_probe_old_filters\"]])\n",
    "\n",
    "# Create Venn Diagram\n",
    "plt.figure(figsize=(8, 8), dpi=200)\n",
    "venn3(\n",
    "    [set_NN, set_melting, set_old_filters],\n",
    "    set_labels=(\"NN\", \"Melting\", \"Old Filters\"),\n",
    "    layout_algorithm=DefaultLayoutAlgorithm(normalize_to=1),\n",
    ")\n",
    "\n",
    "plt.title(\"Venn Diagram of Valid Probes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate self dimers, hairpins and heterodimers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it on your DataFrame `df` (must have a 'padlock' column, and optionally a 'name' column)\n",
    "df = pd.read_csv(\"monahan_panel_barcoded.csv\")\n",
    "result_df = check_padlocks.annotate_with_thermo(df, n_jobs=20)\n",
    "result_df.to_csv(\"monahan_panel_barcoded_with_thermo_trimmed.csv\", index=False)\n",
    "\n",
    "\n",
    "# Left-tail counts beyond 2σ and 3σ for the same three metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), dpi=200)\n",
    "hist_data = [\n",
    "    (df[\"hairpin_dg_kcalmol\"], \"Hairpin ΔG (kcal/mol)\"),\n",
    "    (df[\"homodimer_dg_kcalmol\"], \"Homodimer ΔG (kcal/mol)\"),\n",
    "    (df[\"best_heterodimer_dg_kcalmol\"], \"Best Heterodimer ΔG (kcal/mol)\"),\n",
    "]\n",
    "\n",
    "for ax, (data, title) in zip(axes, hist_data):\n",
    "    data_clean = data.dropna().values\n",
    "\n",
    "    # Plot histogram\n",
    "    if data_clean.size == 0:\n",
    "        ax.set_title(title + \" (no data)\")\n",
    "        ax.set_xlabel(\"ΔG (kcal/mol)\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        continue\n",
    "\n",
    "    n, bins, patches = ax.hist(\n",
    "        data_clean, bins=50, color=\"skyblue\", edgecolor=\"black\", alpha=0.7\n",
    "    )\n",
    "    ax.set_title(title + \" — left tail 2σ/3σ counts\")\n",
    "    ax.set_xlabel(\"ΔG (kcal/mol)\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "\n",
    "    # Mean and sample standard deviation\n",
    "    mu = float(np.mean(data_clean))\n",
    "    sigma = float(np.std(data_clean, ddof=1)) if data_clean.size > 1 else 0.0\n",
    "\n",
    "    y_max = ax.get_ylim()[1]\n",
    "    if sigma > 0:\n",
    "        t2 = mu - 2 * sigma\n",
    "        t3 = mu - 3 * sigma\n",
    "\n",
    "        # Left-tail counts\n",
    "        n2 = int((data_clean <= t2).sum())\n",
    "        n3 = int((data_clean <= t3).sum())\n",
    "\n",
    "        # Annotate thresholds\n",
    "        ax.axvline(t2, color=\"red\", linestyle=\"--\", lw=1, label=\"μ-2σ\")\n",
    "        ax.axvline(t3, color=\"purple\", linestyle=\"--\", lw=1, label=\"μ-3σ\")\n",
    "        ax.text(\n",
    "            t2,\n",
    "            0.9 * y_max,\n",
    "            \"μ-2σ\",\n",
    "            color=\"red\",\n",
    "            rotation=90,\n",
    "            va=\"top\",\n",
    "            ha=\"right\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "        ax.text(\n",
    "            t3,\n",
    "            0.9 * y_max,\n",
    "            \"μ-3σ\",\n",
    "            color=\"purple\",\n",
    "            rotation=90,\n",
    "            va=\"top\",\n",
    "            ha=\"right\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "        # Shade left-tail regions\n",
    "        ax.fill_betweenx([0, y_max], bins[0], t2, color=\"red\", alpha=0.06)\n",
    "        ax.fill_betweenx([0, y_max], bins[0], t3, color=\"purple\", alpha=0.06)\n",
    "\n",
    "        # Display counts\n",
    "        ax.text(\n",
    "            bins[0], 0.80 * y_max, f\"≤ μ-2σ: n={n2}\", color=\"red\", ha=\"left\", fontsize=9\n",
    "        )\n",
    "        ax.text(\n",
    "            bins[0],\n",
    "            0.70 * y_max,\n",
    "            f\"≤ μ-3σ: n={n3}\",\n",
    "            color=\"purple\",\n",
    "            ha=\"left\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "        ax.legend(loc=\"upper right\", fontsize=8)\n",
    "    else:\n",
    "        ax.text(\n",
    "            0.5, 0.5, \"σ = 0 or insufficient data\", transform=ax.transAxes, ha=\"center\"\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute and plot graph of worst heterodimer offenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heterodimer interaction network: highlight oligos responsible for strongest interactions (with progress + faster layout options + outlier filtering + log-scale bar + log colourscale)\n",
    "import time\n",
    "from ast import literal_eval\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import cm, colors\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "try:\n",
    "    import networkx as nx\n",
    "except ImportError:\n",
    "    raise RuntimeError(\n",
    "        \"networkx is required for this visualization. Please install it (e.g., pip install networkx).\"\n",
    "    )\n",
    "\n",
    "start_all = time.perf_counter()\n",
    "print(\"[0/8] Starting heterodimer network build...\", flush=True)\n",
    "\n",
    "# Config knobs\n",
    "use_percentile_threshold = True  # if True, use percentile instead of fixed threshold\n",
    "percentile_cut = 5  # keep edges at or below this ΔG percentile (most negative)\n",
    "fixed_threshold_dg = -8.0  # kcal/mol, used if use_percentile_threshold is False\n",
    "top_k = 100  # number of top offenders to list/bar-plot\n",
    "\n",
    "# Visualization performance knobs\n",
    "layout_mode = \"auto\"  # one of: \"auto\"|\"sfdp\"|\"fa2\"|\"spring\"|\"spectral\"\n",
    "spring_iterations = 20  # fewer iterations to speed up\n",
    "limit_nodes_for_plot = True  # draw only the top-N offending oligos to speed up layout\n",
    "max_nodes_to_draw = 1000  # if graph is larger than this, plot a top-N subgraph\n",
    "\n",
    "# Layout outlier filtering (post-layout)\n",
    "hide_position_outliers = (\n",
    "    True  # if True, drop nodes with extreme x/y positions from the drawn graph\n",
    ")\n",
    "pos_outlier_quantile = 0.005  # drop nodes outside these lower/upper quantiles on x or y\n",
    "\n",
    "# Expected columns\n",
    "col_name = \"name\"\n",
    "col_partners = \"heterodimer_candidate_partners\"\n",
    "col_dgs = \"heterodimer_candidate_dgs_kcalmol\"\n",
    "\n",
    "\n",
    "# Safety: turn potential stringified lists back into Python lists\n",
    "def ensure_list(x):\n",
    "    if isinstance(x, (list, tuple, np.ndarray)):\n",
    "        return list(x)\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            val = literal_eval(x)\n",
    "            return list(val) if isinstance(val, (list, tuple, np.ndarray)) else []\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "\n",
    "# Sanity checks\n",
    "if not set([col_name, col_partners, col_dgs]).issubset(df.columns):\n",
    "    missing = set([col_name, col_partners, col_dgs]) - set(df.columns)\n",
    "    raise ValueError(f\"Missing required columns in df: {missing}\")\n",
    "print(f\"DataFrame rows: {len(df):,}\", flush=True)\n",
    "\n",
    "# Stage 1: Build an undirected edge list with best (most negative) ΔG per pair\n",
    "t0 = time.perf_counter()\n",
    "print(\"[1/8] Parsing candidates and computing best ΔG per oligo pair...\", flush=True)\n",
    "edge_best_dg = {}  # key: (a,b) sorted tuple, value: most negative dg\n",
    "names_seen = set()\n",
    "\n",
    "for _, row in tqdm(\n",
    "    df.iterrows(), total=len(df), desc=\"Build best ΔG per pair\", leave=False\n",
    "):\n",
    "    a = row[col_name]\n",
    "    names_seen.add(a)\n",
    "    partners = ensure_list(row[col_partners])\n",
    "    dgs = ensure_list(row[col_dgs])\n",
    "    for b, dg in zip(partners, dgs):\n",
    "        if b is None or pd.isna(b) or a == b:\n",
    "            continue\n",
    "        try:\n",
    "            dg_val = float(dg)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not np.isfinite(dg_val):\n",
    "            continue\n",
    "        pair = tuple(sorted((a, b)))\n",
    "        if pair not in edge_best_dg or dg_val < edge_best_dg[pair]:\n",
    "            edge_best_dg[pair] = dg_val\n",
    "print(\n",
    "    f\"  Unique pairs seen: {len(edge_best_dg):,}; unique oligos: {len(names_seen):,}\",\n",
    "    flush=True,\n",
    ")\n",
    "print(f\"  Stage 1 took {time.perf_counter()-t0:.2f}s\", flush=True)\n",
    "\n",
    "if not edge_best_dg:\n",
    "    raise ValueError(\n",
    "        \"No heterodimer candidate edges found after parsing. Check the input columns.\"\n",
    "    )\n",
    "\n",
    "# Stage 2: Compute threshold\n",
    "t0 = time.perf_counter()\n",
    "print(\"[2/8] Computing strong-interaction threshold...\", flush=True)\n",
    "all_dgs = np.array(list(edge_best_dg.values()), dtype=float)\n",
    "if use_percentile_threshold:\n",
    "    threshold_dg = np.percentile(all_dgs, percentile_cut)\n",
    "    print(\n",
    "        f\"  Using percentile cut {percentile_cut}% -> threshold ΔG ≤ {threshold_dg:.3f} kcal/mol\",\n",
    "        flush=True,\n",
    "    )\n",
    "else:\n",
    "    threshold_dg = fixed_threshold_dg\n",
    "    print(f\"  Using fixed threshold ΔG ≤ {threshold_dg:.3f} kcal/mol\", flush=True)\n",
    "\n",
    "# Stage 3: Keep only strong interactions\n",
    "print(\"[3/8] Selecting strong edges...\", flush=True)\n",
    "strong_edges = {pair: dg for pair, dg in edge_best_dg.items() if dg <= threshold_dg}\n",
    "print(\n",
    "    f\"  Strong edges: {len(strong_edges):,} out of {len(edge_best_dg):,} total pairs\",\n",
    "    flush=True,\n",
    ")\n",
    "print(f\"  Stage 2+3 took {time.perf_counter()-t0:.2f}s\", flush=True)\n",
    "\n",
    "# Stage 4: Build graph with strong edges only\n",
    "t0 = time.perf_counter()\n",
    "print(\"[4/8] Building graph from strong edges...\", flush=True)\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(names_seen)\n",
    "for (u, v), dg in tqdm(\n",
    "    strong_edges.items(), total=len(strong_edges), desc=\"Add strong edges\", leave=False\n",
    "):\n",
    "    strength = -dg  # more negative dg => larger strength\n",
    "    G.add_edge(u, v, dg=dg, strength=strength)\n",
    "print(\n",
    "    f\"  Graph now has {G.number_of_nodes():,} nodes and {G.number_of_edges():,} edges\",\n",
    "    flush=True,\n",
    ")\n",
    "print(f\"  Stage 4 took {time.perf_counter()-t0:.2f}s\", flush=True)\n",
    "\n",
    "# Stage 5: Compute offender score per node\n",
    "t0 = time.perf_counter()\n",
    "print(\"[5/8] Computing offense scores...\", flush=True)\n",
    "offense_score = {n: 0.0 for n in G.nodes()}\n",
    "for u, v, data in tqdm(\n",
    "    G.edges(data=True),\n",
    "    total=G.number_of_edges(),\n",
    "    desc=\"Accumulate edge strengths\",\n",
    "    leave=False,\n",
    "):\n",
    "    s = data.get(\"strength\", 0.0)\n",
    "    offense_score[u] += s\n",
    "    offense_score[v] += s\n",
    "isolates = [n for n in G.nodes() if G.degree(n) == 0]\n",
    "G.remove_nodes_from(isolates)\n",
    "for n in isolates:\n",
    "    offense_score.pop(n, None)\n",
    "print(\n",
    "    f\"  Removed isolates: {len(isolates):,}; remaining nodes: {G.number_of_nodes():,}\",\n",
    "    flush=True,\n",
    ")\n",
    "print(f\"  Stage 5 took {time.perf_counter()-t0:.2f}s\", flush=True)\n",
    "\n",
    "if G.number_of_edges() == 0:\n",
    "    raise ValueError(\n",
    "        \"No edges passed the strong-interaction threshold. Try relaxing the threshold or percentile.\"\n",
    "    )\n",
    "\n",
    "# Stage 6: Prepare node sizes and colors (for the graph we'll draw)\n",
    "t0 = time.perf_counter()\n",
    "print(\"[6/8] Preparing node sizes/colors...\", flush=True)\n",
    "# Optionally focus visualization on top-N offenders to speed up layout\n",
    "draw_nodes = list(G.nodes())\n",
    "if limit_nodes_for_plot and len(G) > max_nodes_to_draw:\n",
    "    draw_nodes = [\n",
    "        n\n",
    "        for n, _ in sorted(offense_score.items(), key=lambda x: x[1], reverse=True)[\n",
    "            :max_nodes_to_draw\n",
    "        ]\n",
    "    ]\n",
    "    print(\n",
    "        f\"  Limiting draw to top-{len(draw_nodes)} nodes by offense score (of {len(G):,})\",\n",
    "        flush=True,\n",
    "    )\n",
    "draw_G = G.subgraph(draw_nodes).copy()\n",
    "\n",
    "scores = np.array([offense_score[n] for n in draw_G.nodes()], dtype=float)\n",
    "if scores.size == 0:\n",
    "    raise ValueError(\n",
    "        \"No nodes with offense scores in draw graph. Check the thresholds.\"\n",
    "    )\n",
    "score_max = float(scores.max())\n",
    "score_range = score_max if score_max > 0 else 1.0\n",
    "node_sizes = {n: 80 + 520 * (offense_score[n] / score_range) for n in draw_G.nodes()}\n",
    "cmap = cm.get_cmap(\"Reds\")\n",
    "\n",
    "# Use a log-based colour scale if possible, otherwise fall back to linear\n",
    "used_log_scale = False\n",
    "if np.any(scores > 0) and score_max > 0:\n",
    "    min_pos = float(scores[scores > 0].min())\n",
    "    vmin_log = max(min_pos, score_max * 1e-6)  # ensure strictly > 0 and < vmax\n",
    "    if vmin_log < score_max:\n",
    "        norm = colors.LogNorm(vmin=vmin_log, vmax=score_max)\n",
    "        used_log_scale = True\n",
    "    else:\n",
    "        norm = colors.Normalize(vmin=0, vmax=score_max)\n",
    "else:\n",
    "    norm = colors.Normalize(vmin=0, vmax=max(1.0, score_max))\n",
    "\n",
    "node_colors = {n: cmap(norm(offense_score[n])) for n in draw_G.nodes()}\n",
    "edge_strengths = np.array(\n",
    "    [data[\"strength\"] for _, _, data in draw_G.edges(data=True)], dtype=float\n",
    ")\n",
    "s_max = float(edge_strengths.max()) if edge_strengths.size else 1.0\n",
    "edge_widths = [\n",
    "    0.5 + 3.0 * (data[\"strength\"] / s_max) for _, _, data in draw_G.edges(data=True)\n",
    "]\n",
    "print(\n",
    "    f\"  Draw graph: {draw_G.number_of_nodes():,} nodes, {draw_G.number_of_edges():,} edges\",\n",
    "    flush=True,\n",
    ")\n",
    "print(f\"  Score max: {score_max:.3f}; edge strength max: {s_max:.3f}\", flush=True)\n",
    "print(f\"  Stage 6 took {time.perf_counter()-t0:.2f}s\", flush=True)\n",
    "\n",
    "# Stage 7: Layout (choose a faster option if available)\n",
    "t0 = time.perf_counter()\n",
    "print(\"[7/8] Computing layout...\", flush=True)\n",
    "chosen_mode = layout_mode\n",
    "pos = None\n",
    "if chosen_mode == \"auto\":\n",
    "    # Prefer Graphviz sfdp if available (good for large graphs), else try FA2, else faster spring, else spectral\n",
    "    try:\n",
    "        from networkx.drawing.nx_agraph import \\\n",
    "            graphviz_layout  # requires pygraphviz + graphviz\n",
    "        pos = graphviz_layout(draw_G, prog=\"sfdp\")\n",
    "        chosen_mode = \"sfdp\"\n",
    "    except Exception:\n",
    "        try:\n",
    "            from fa2 import ForceAtlas2  # pip install fa2\n",
    "\n",
    "            fa2 = ForceAtlas2(\n",
    "                outboundAttractionDistribution=False,  # FA2 defaults\n",
    "                edgeWeightInfluence=1.0,\n",
    "                jitterTolerance=1.0,\n",
    "                barnesHutOptimize=True,\n",
    "                barnesHutTheta=1.2,\n",
    "                scalingRatio=2.0,\n",
    "                gravity=1.0,\n",
    "                verbose=False,\n",
    "            )\n",
    "            pos_array = fa2.forceatlas2_networkx_layout(\n",
    "                draw_G, pos=None, iterations=300\n",
    "            )\n",
    "            pos = pos_array\n",
    "            chosen_mode = \"fa2\"\n",
    "        except Exception:\n",
    "            try:\n",
    "                # Use spectral to initialize, then a few spring iterations for refinement\n",
    "                pos_init = nx.spectral_layout(draw_G)\n",
    "                pos = nx.spring_layout(\n",
    "                    draw_G,\n",
    "                    seed=42,\n",
    "                    weight=\"strength\",\n",
    "                    iterations=spring_iterations,\n",
    "                    pos=pos_init,\n",
    "                )\n",
    "                chosen_mode = \"spring+spectral-init\"\n",
    "            except Exception:\n",
    "                pos = nx.spring_layout(\n",
    "                    draw_G, seed=42, weight=\"strength\", iterations=spring_iterations\n",
    "                )\n",
    "                chosen_mode = \"spring\"\n",
    "elif chosen_mode == \"sfdp\":\n",
    "    from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "    pos = graphviz_layout(draw_G, prog=\"sfdp\")\n",
    "elif chosen_mode == \"fa2\":\n",
    "    from fa2 import ForceAtlas2\n",
    "\n",
    "    fa2 = ForceAtlas2(\n",
    "        outboundAttractionDistribution=False,\n",
    "        edgeWeightInfluence=1.0,\n",
    "        jitterTolerance=1.0,\n",
    "        barnesHutOptimize=True,\n",
    "        barnesHutTheta=1.2,\n",
    "        scalingRatio=2.0,\n",
    "        gravity=1.0,\n",
    "        verbose=False,\n",
    "    )\n",
    "    pos = fa2.forceatlas2_networkx_layout(draw_G, pos=None, iterations=300)\n",
    "elif chosen_mode == \"spring\":\n",
    "    pos = nx.spring_layout(\n",
    "        draw_G, seed=42, weight=\"strength\", iterations=spring_iterations\n",
    "    )\n",
    "elif chosen_mode == \"spectral\":\n",
    "    pos = nx.spectral_layout(draw_G)\n",
    "else:\n",
    "    pos = nx.spring_layout(\n",
    "        draw_G, seed=42, weight=\"strength\", iterations=spring_iterations\n",
    "    )\n",
    "    chosen_mode = \"spring\"\n",
    "print(f\"  Layout mode: {chosen_mode}\", flush=True)\n",
    "print(f\"  Stage 7 took {time.perf_counter()-t0:.2f}s\", flush=True)\n",
    "\n",
    "# Optional Stage 7b: Filter positional outliers to avoid squashed views\n",
    "if hide_position_outliers and len(draw_G) > 0:\n",
    "    print(\"[7b] Filtering positional outliers from layout...\", flush=True)\n",
    "    xs = np.array([pos[n][0] for n in draw_G.nodes()], dtype=float)\n",
    "    ys = np.array([pos[n][1] for n in draw_G.nodes()], dtype=float)\n",
    "    q = float(pos_outlier_quantile)\n",
    "    x_lo, x_hi = np.quantile(xs, [q, 1 - q])\n",
    "    y_lo, y_hi = np.quantile(ys, [q, 1 - q])\n",
    "    keep_nodes = [\n",
    "        n\n",
    "        for n in draw_G.nodes()\n",
    "        if (x_lo <= pos[n][0] <= x_hi) and (y_lo <= pos[n][1] <= y_hi)\n",
    "    ]\n",
    "    removed = draw_G.number_of_nodes() - len(keep_nodes)\n",
    "    if removed > 0:\n",
    "        draw_G = draw_G.subgraph(keep_nodes).copy()\n",
    "        # Filter visuals and recompute edge widths using the new subgraph\n",
    "        node_sizes = {n: node_sizes[n] for n in draw_G.nodes()}\n",
    "        node_colors = {n: node_colors[n] for n in draw_G.nodes()}\n",
    "        pos = {n: pos[n] for n in draw_G.nodes()}\n",
    "        edge_strengths = np.array(\n",
    "            [data[\"strength\"] for _, _, data in draw_G.edges(data=True)], dtype=float\n",
    "        )\n",
    "        s_max = float(edge_strengths.max()) if edge_strengths.size else 1.0\n",
    "        edge_widths = [\n",
    "            0.5 + 3.0 * (data[\"strength\"] / s_max)\n",
    "            for _, _, data in draw_G.edges(data=True)\n",
    "        ]\n",
    "        print(\n",
    "            f\"  Removed {removed} positional outliers (kept {len(keep_nodes)} nodes)\",\n",
    "            flush=True,\n",
    "        )\n",
    "    else:\n",
    "        print(\"  No positional outliers removed\", flush=True)\n",
    "\n",
    "# Stage 8: Render\n",
    "t0 = time.perf_counter()\n",
    "print(\"[8/8] Rendering network and top-offenders plot...\", flush=True)\n",
    "fig = plt.figure(figsize=(16, 12), dpi=200)\n",
    "gs = fig.add_gridspec(1, 2, width_ratios=[2, 1])\n",
    "ax_net = fig.add_subplot(gs[0, 0])\n",
    "ax_bar = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "nx.draw_networkx_edges(\n",
    "    draw_G, pos, ax=ax_net, width=edge_widths, edge_color=\"gray\", alpha=0.6\n",
    ")\n",
    "nx.draw_networkx_nodes(\n",
    "    draw_G,\n",
    "    pos,\n",
    "    ax=ax_net,\n",
    "    node_size=[node_sizes[n] for n in draw_G.nodes()],\n",
    "    node_color=[node_colors[n] for n in draw_G.nodes()],\n",
    "    linewidths=0.3,\n",
    "    edgecolors=\"black\",\n",
    "    alpha=0.95,\n",
    ")\n",
    "\n",
    "top_labels = sorted(\n",
    "    ((n, offense_score[n]) for n in draw_G.nodes()), key=lambda x: x[1], reverse=True\n",
    ")[:10]\n",
    "label_nodes = {n for n, _ in top_labels}\n",
    "labels = {n: (n if n in label_nodes else \"\") for n in draw_G.nodes()}\n",
    "nx.draw_networkx_labels(draw_G, pos, labels=labels, font_size=8, ax=ax_net)\n",
    "\n",
    "sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=ax_net, fraction=0.046, pad=0.04)\n",
    "cbar.set_label(\n",
    "    \"Offense score (sum of strong interaction strengths\"\n",
    "    + (\"; log scale)\" if used_log_scale else \")\")\n",
    ")\n",
    "\n",
    "ax_net.set_title(\n",
    "    f\"Heterodimer interaction network (drawn: {draw_G.number_of_nodes()} nodes)\\nStrong edges: ΔG ≤ {threshold_dg:.2f} kcal/mol; nodes sized/colored by offense score\"\n",
    ")\n",
    "ax_net.axis(\"off\")\n",
    "\n",
    "top_series = pd.Series(offense_score).sort_values(ascending=False).head(top_k)\n",
    "top_series.plot(kind=\"barh\", ax=ax_bar, color=\"crimson\", alpha=0.8)\n",
    "ax_bar.invert_yaxis()\n",
    "ax_bar.set_xscale(\"log\")  # log scale for offense scores\n",
    "ax_bar.set_xlabel(\"Offense score (sum of -ΔG over strong edges, log scale)\")\n",
    "ax_bar.set_title(f\"Top {min(top_k, len(top_series))} offending oligos\")\n",
    "ax_bar.grid(axis=\"x\", which=\"both\", linestyle=\"--\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"  Stage 8 took {time.perf_counter()-t0:.2f}s\", flush=True)\n",
    "\n",
    "# Summary and optional export (still computed on the full strong-edge graph G)\n",
    "edges_export = (\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"oligo_a\": a,\n",
    "                \"oligo_b\": b,\n",
    "                \"dg_kcalmol\": data[\"dg\"],\n",
    "                \"strength\": data[\"strength\"],\n",
    "            }\n",
    "            for a, b, data in G.edges(data=True)\n",
    "        ]\n",
    "    )\n",
    "    .sort_values(\"dg_kcalmol\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(\n",
    "    f\"Done in {time.perf_counter()-start_all:.2f}s. Strong interactions: {len(edges_export)} edges among {G.number_of_nodes()} nodes (threshold ΔG ≤ {threshold_dg:.2f}).\",\n",
    "    flush=True,\n",
    ")\n",
    "# edges_export.to_csv('strong_heterodimer_edges.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to filter probes based solely on hairpin, homodimer and heterodimer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heterodimer-aware filtering: keep up to N padlocks per gene using offense score + homo/hairpin ΔG\n",
    "\n",
    "# --- Parameters ---\n",
    "keep_per_gene = 10  # target per gene\n",
    "\n",
    "# Choose filtering method: 'two_stage' or 'combined'\n",
    "method = \"combined\"  # or 'combined'\n",
    "\n",
    "# Columns\n",
    "name_col = \"name\"\n",
    "gene_col = \"gene_name\"\n",
    "homodimer_col = \"homodimer_dg_kcalmol\"\n",
    "hairpin_col = \"hairpin_dg_kcalmol\"\n",
    "heterodimer_candidates = [\"best_heterodimer_dg_kcalmol\", \"heterodimer_dg_kcalmol\"]\n",
    "heterodimer_col = next((c for c in heterodimer_candidates if c in df.columns), None)\n",
    "\n",
    "# Two-stage config (used when method == 'two_stage')\n",
    "drop_top_offenders_n = 1000  # drop this many worst global offenders (by offense_score)\n",
    "drop_top_offenders_frac = (\n",
    "    0.0  # or drop this fraction (0..1) of unique names; ignored if n > 0\n",
    ")\n",
    "\n",
    "# Per-gene weights (ranks)\n",
    "w_homo = 1.0\n",
    "w_hair = 1.0\n",
    "w_offense = 1.0  # only used for method == 'combined'\n",
    "\n",
    "# Offense score requirements\n",
    "require_offense = True  # if True, error if offense_score is not available\n",
    "offense_default = 0.0  # default if not required\n",
    "\n",
    "# --- Validation & setup ---\n",
    "# Fallback for gene column\n",
    "if gene_col not in df.columns:\n",
    "    for alt in [\"gene\", \"padlock_target_gene\", \"query\"]:\n",
    "        if alt in df.columns:\n",
    "            gene_col = alt\n",
    "            break\n",
    "if gene_col not in df.columns:\n",
    "    raise ValueError(\n",
    "        f\"gene column '{gene_col}' not found; tried fallbacks ['gene','padlock_target_gene','query']\"\n",
    "    )\n",
    "\n",
    "needed = [name_col, gene_col, homodimer_col, hairpin_col]\n",
    "if heterodimer_col is None:\n",
    "    raise ValueError(\n",
    "        \"No heterodimer ΔG column found (expected one of: \"\n",
    "        + \", \".join(heterodimer_candidates)\n",
    "        + \")\"\n",
    "    )\n",
    "needed.append(heterodimer_col)\n",
    "missing = [c for c in needed if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "# Bring offense_score into the DataFrame\n",
    "if \"offense_score\" in globals():\n",
    "    df = df.copy()\n",
    "    df[\"offense_score\"] = df[name_col].map(offense_score).fillna(offense_default)\n",
    "elif require_offense:\n",
    "    raise ValueError(\n",
    "        \"offense_score not found in the notebook environment. Run the heterodimer network cell first.\"\n",
    "    )\n",
    "else:\n",
    "    df = df.copy()\n",
    "    df[\"offense_score\"] = offense_default\n",
    "\n",
    "# Precompute a global offense penalty in [0,1] (higher is worse)\n",
    "# Rank worst offenders as highest penalty\n",
    "if len(df) > 1:\n",
    "    offense_rank = df[\"offense_score\"].rank(\n",
    "        ascending=False, method=\"dense\"\n",
    "    )  # 1 = worst offender\n",
    "    max_rank = float(offense_rank.max()) if offense_rank.notna().any() else 1.0\n",
    "    df[\"_offense_penalty\"] = (offense_rank - 1.0) / max(1.0, max_rank - 1.0)\n",
    "else:\n",
    "    df[\"_offense_penalty\"] = 0.0\n",
    "\n",
    "\n",
    "# --- Helpers ---\n",
    "def _rank_per_gene(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = g.copy()\n",
    "    # Convert and fill NaNs; very negative means very strong (bad), so we want higher better\n",
    "    g[\"_hom\"] = pd.to_numeric(g[homodimer_col], errors=\"coerce\")\n",
    "    g[\"_hai\"] = pd.to_numeric(g[hairpin_col], errors=\"coerce\")\n",
    "    g[\"_het\"] = pd.to_numeric(g[heterodimer_col], errors=\"coerce\")\n",
    "    g[[\"_hom\", \"_hai\", \"_het\"]] = g[[\"_hom\", \"_hai\", \"_het\"]].fillna(-np.inf)\n",
    "    # Rank within gene: higher ΔG (weaker) should get smaller (better) rank index for selection\n",
    "    # We will normalize ranks later when needed.\n",
    "    g[\"_rank_hom\"] = g[\"_hom\"].rank(ascending=False, method=\"dense\")\n",
    "    g[\"_rank_hai\"] = g[\"_hai\"].rank(ascending=False, method=\"dense\")\n",
    "    return g\n",
    "\n",
    "\n",
    "def _select_by_two_stage(g: pd.DataFrame, k: int) -> pd.DataFrame:\n",
    "    g = _rank_per_gene(g)\n",
    "    # Combined score using homodimer/hairpin only (offense removed already at stage 1)\n",
    "    g[\"_score\"] = w_homo * g[\"_rank_hom\"] + w_hair * g[\"_rank_hai\"]\n",
    "    g = g.sort_values(\n",
    "        [\"_score\", homodimer_col, hairpin_col], ascending=[True, False, False]\n",
    "    )\n",
    "    return g.head(k)\n",
    "\n",
    "\n",
    "def _select_by_combined(g: pd.DataFrame, k: int) -> pd.DataFrame:\n",
    "    g = _rank_per_gene(g)\n",
    "    # Normalize per-gene ranks to [0,1] to be comparable with offense penalty\n",
    "    n = max(1.0, float(len(g) - 1))\n",
    "    g[\"_rank_hom_n\"] = (g[\"_rank_hom\"] - 1.0) / n\n",
    "    g[\"_rank_hai_n\"] = (g[\"_rank_hai\"] - 1.0) / n\n",
    "    # Global offense penalty already in df; merge in\n",
    "    # Compute composite: lower is better\n",
    "    g[\"_combo\"] = (\n",
    "        w_homo * g[\"_rank_hom_n\"]\n",
    "        + w_hair * g[\"_rank_hai_n\"]\n",
    "        + w_offense * g[\"_offense_penalty\"]\n",
    "    )\n",
    "    g = g.sort_values(\n",
    "        [\"_combo\", homodimer_col, hairpin_col, heterodimer_col],\n",
    "        ascending=[True, False, False, False],\n",
    "    )\n",
    "    return g.head(k)\n",
    "\n",
    "\n",
    "# --- Two-stage prefilter (drop global worst offenders) ---\n",
    "dff = df.copy()\n",
    "dropped_names = []\n",
    "if method == \"two_stage\":\n",
    "    # Determine offenders to drop\n",
    "    unique_names = (\n",
    "        dff[[name_col, \"offense_score\"]]\n",
    "        .drop_duplicates(subset=[name_col])\n",
    "        .sort_values(\"offense_score\", ascending=False)\n",
    "    )\n",
    "    if drop_top_offenders_n and drop_top_offenders_n > 0:\n",
    "        to_drop = unique_names.head(int(drop_top_offenders_n))[name_col].tolist()\n",
    "    elif drop_top_offenders_frac and 0 < drop_top_offenders_frac < 1:\n",
    "        m = int(len(unique_names) * drop_top_offenders_frac)\n",
    "        to_drop = unique_names.head(m)[name_col].tolist()\n",
    "    else:\n",
    "        to_drop = []\n",
    "    if to_drop:\n",
    "        dropped_names = to_drop\n",
    "        dff = dff[~dff[name_col].isin(to_drop)]\n",
    "        print(f\"Dropped {len(to_drop)} global worst offenders (by offense_score)\")\n",
    "\n",
    "# --- Apply per-gene selection ---\n",
    "sizes = dff.groupby(gene_col, dropna=False).size()\n",
    "genes_over = sizes[sizes > keep_per_gene].index\n",
    "genes_small = sizes[sizes <= keep_per_gene].index\n",
    "\n",
    "kept_frames = []\n",
    "if len(genes_small) > 0:\n",
    "    kept_frames.append(dff[dff[gene_col].isin(genes_small)])\n",
    "if len(genes_over) > 0:\n",
    "    if method == \"two_stage\":\n",
    "        selected = (\n",
    "            dff[dff[gene_col].isin(genes_over)]\n",
    "            .groupby(gene_col, group_keys=False)\n",
    "            .apply(lambda g: _select_by_two_stage(g, keep_per_gene))\n",
    "        )\n",
    "    elif method == \"combined\":\n",
    "        selected = (\n",
    "            dff[dff[gene_col].isin(genes_over)]\n",
    "            .groupby(gene_col, group_keys=False)\n",
    "            .apply(lambda g: _select_by_combined(g, keep_per_gene))\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unknown method; use 'two_stage' or 'combined'\")\n",
    "    kept_frames.append(selected)\n",
    "\n",
    "df_kept = pd.concat(kept_frames, axis=0).sort_index()\n",
    "df_removed2 = dff.drop(df_kept.index)\n",
    "\n",
    "# --- Summaries ---\n",
    "total_genes = int(df[gene_col].nunique(dropna=False))\n",
    "print(f\"Genes total: {total_genes}\")\n",
    "print(f\"Method: {method}\")\n",
    "if method == \"two_stage\":\n",
    "    print(f\"Global offenders dropped: {len(dropped_names)}\")\n",
    "print(f\"Padlocks kept: {len(df_kept)} / {len(df)} (removed {len(df) - len(df_kept)})\")\n",
    "print(\n",
    "    \"Kept per gene (top 10):\\n\",\n",
    "    df_kept.groupby(gene_col, dropna=False)\n",
    "    .size()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10),\n",
    ")\n",
    "\n",
    "# Optional outputs\n",
    "# df_kept.to_csv('padlocks_filtered_kept_heteroaware.csv', index=False)\n",
    "# df_removed2.to_csv('padlocks_filtered_removed_heteroaware.csv', index=False)\n",
    "\n",
    "# To make the filtered set your working df, uncomment:\n",
    "# df = df_kept.copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-padlock-design",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
