{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf55742",
   "metadata": {},
   "source": [
    "# Process old probes and blast them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd659f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "\n",
    "import config\n",
    "import lib.checkinput as checkinput\n",
    "import lib.finalizeprobes as finalizeprobes\n",
    "import lib.formatrefseq as formatrefseq\n",
    "import lib.makesnails as makesnails\n",
    "import lib.parblast as parblast\n",
    "import lib.parmsa as parmsa\n",
    "import lib.readblast as readblast\n",
    "import lib.readfastafile as readfastafile\n",
    "import lib.retrieveseq as retrieveseq\n",
    "import lib.screenseq as screenseq\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b37cb5",
   "metadata": {},
   "source": [
    "## Read binding sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a3aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "probeset = pd.read_csv(\"pilot_snail_probes.csv\")\n",
    "probeset = probeset.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba666663",
   "metadata": {},
   "outputs": [],
   "source": [
    "probeset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955895db",
   "metadata": {},
   "outputs": [],
   "source": [
    "primer = []\n",
    "melting_temp = []\n",
    "\n",
    "for i, row in probeset.iterrows():\n",
    "    number = int(row[\"PROBE\"][-2:])\n",
    "    if number < 10:\n",
    "        continue\n",
    "    primer.append(row[\"SEQUENCE\"][-12:])\n",
    "    melting_temp.append(readblast.calc_tm_NN(primer[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e2cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(melting_temp)\n",
    "plt.xlabel(\"Melting temperature of 3' primer\")\n",
    "plt.ylabel(\"Number of probes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7af2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "probeset[\"SEQUENCE\"][i][\n",
    "    12 : -int(31 + probeset[\"Total pad\"][i])\n",
    "]  # assuming 6bp binding region at the begginning,\n",
    "# 6bp at the end for binding, barcode of 7 and primer of 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c8e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = \"/5Phos/GGGCACTCGAGTTGGCAGACCTCTGCGTCGGACTGTAGAACTCTATTAGTTTCGCACAA\"\n",
    "binding = probe[13:-33]\n",
    "binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "genelist = list(np.ones(len(probeset)))\n",
    "bindlist = list(np.ones(len(probeset)))\n",
    "for row in probeset.iterrows():\n",
    "    index = row[0]\n",
    "    row = row[1]\n",
    "    if row[\"PROBE\"][-2] == \"0\":\n",
    "        id = \"padlock\"\n",
    "        binding = row[\"SEQUENCE\"][12 : -int(31 + row[\"Total pad\"])]\n",
    "    elif row[\"PROBE\"][-2] == \"1\":\n",
    "        id = \"primer\"\n",
    "        binding = row[\"SEQUENCE\"][0:-13]\n",
    "    else:\n",
    "        print(\"catastrophic error\")\n",
    "    genelist[index] = row[\"PROBE\"]\n",
    "    bindlist[index] = binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933427ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "bindset = pd.DataFrame.from_dict({\"probe\": genelist, \"binding\": bindlist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfbcdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bindset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_probe_primer(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a dataframe with columns ['probe', 'binding'] where probe names look like 'Gad1_01', 'Gad1_11', etc.,\n",
    "    create pairs (01↔11, 02↔12, ...) per gene, concatenate binding as primer+probe, and\n",
    "    return a dataframe with columns ['probe', 'binding'] named like 'Gad1_1'.\n",
    "    \"\"\"\n",
    "    # Parse gene and numeric index from probe names\n",
    "    m = df[\"probe\"].str.extract(r\"^(?P<gene>.+)_(?P<idx>\\d+)$\")\n",
    "    if m.isna().any().any():\n",
    "        raise ValueError(\n",
    "            \"Some probe names do not match the expected pattern '<gene>_<2-digit index>'\"\n",
    "        )\n",
    "\n",
    "    work = df.copy()\n",
    "    work[\"gene\"] = m[\"gene\"]\n",
    "    work[\"idx\"] = m[\"idx\"].astype(int)\n",
    "    print(work)\n",
    "\n",
    "    # Split into \"probe\" (0k) and \"primer\" (1k). We treat idx 0-9 as probes, 10-19 as primers.\n",
    "    # If your scheme extends beyond 19, this still generalizes because we match (k, k+10).\n",
    "    probes = work[work[\"idx\"] < 10].rename(columns={\"binding\": \"probe_seq\"})\n",
    "    primers = work[work[\"idx\"] >= 10].rename(columns={\"binding\": \"primer_seq\"})\n",
    "    print(probes)\n",
    "    print(primers)\n",
    "    # Create k for matching (k pairs with k+10)\n",
    "    probes[\"k\"] = probes[\"idx\"]  # 01 -> k=1, 02 -> k=2, ...\n",
    "    primers[\"k\"] = primers[\"idx\"] - 10  # 11 -> k=1, 12 -> k=2, ...\n",
    "\n",
    "    # Join on (gene, k)\n",
    "    merged = pd.merge(\n",
    "        probes[[\"gene\", \"k\", \"probe\", \"probe_seq\"]],\n",
    "        primers[[\"gene\", \"k\", \"probe\", \"primer_seq\"]],\n",
    "        on=[\"gene\", \"k\"],\n",
    "        how=\"inner\",\n",
    "        validate=\"one_to_one\",\n",
    "    )\n",
    "\n",
    "    # Concatenate sequences: primer then probe\n",
    "    merged[\"binding\"] = merged[\"primer_seq\"] + merged[\"probe_seq\"]\n",
    "\n",
    "    # Name as '<gene>_<k>' without leading zeros (keep original gene casing)\n",
    "    merged[\"probe\"] = merged.apply(lambda r: f\"{r['gene']}_{int(r['k'])}\", axis=1)\n",
    "\n",
    "    # Final tidy dataframe\n",
    "    out = merged[[\"probe\", \"binding\"]].sort_values([\"probe\"]).reset_index(drop=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b06bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sequences = merge_probe_primer(bindset)\n",
    "merged_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8436f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sequences[\"length\"] = [len(s) for s in merged_sequences[\"binding\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da43cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(merged_sequences[\"length\"], bins=20)\n",
    "plt.title(\"DNA binding length\")\n",
    "plt.xlabel(\"Length (bp)\")\n",
    "plt.ylabel(\"Number of probes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8832faa",
   "metadata": {},
   "source": [
    "Some are exceptionally short!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16cf6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = \"AAATTTT\"\n",
    "\n",
    "\n",
    "def revcomp(seq):\n",
    "    complement = str.maketrans(\"ATCG\", \"TAGC\")\n",
    "    return seq.translate(complement)[::-1]\n",
    "\n",
    "\n",
    "revcomp(probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e753db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sequences[\"target_rc\"] = merged_sequences[\"binding\"].apply(revcomp)\n",
    "merged_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319d37f",
   "metadata": {},
   "source": [
    "## Blasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667eedb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to be safe:\n",
    "BASE_DIR = os.path.abspath(\"/nemo/lab/znamenskiyp/home/shared/resources/\")\n",
    "reference_transcriptome = \"ensembl\"\n",
    "\n",
    "\n",
    "blast_db_file = os.path.join(BASE_DIR, reference_transcriptome, \"mouse.selected\")\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def write_blast_script(\n",
    "    df,\n",
    "    db_path,\n",
    "    outdir=\"blast_results\",\n",
    "    script_name=\"run_blast.sh\",\n",
    "    word_size=7,\n",
    "    strand=\"plus\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Write a shell script with blastn commands for each probe sequence.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): must have columns ['probe','binding']\n",
    "        db_path (str): path to blast database (without suffixes)\n",
    "        outdir (str): directory for fasta and result files\n",
    "        script_name (str): name of the shell script\n",
    "        word_size (int): blastn word size\n",
    "        strand (str): strand option for blastn\n",
    "    \"\"\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    script_path = os.path.join(outdir, script_name)\n",
    "\n",
    "    with open(script_path, \"w\") as sh:\n",
    "        sh.write(\"#!/bin/bash\\n\\n\")\n",
    "        sh.write(\n",
    "            \"#SBATCH --job-name=blast_job\\n\"\n",
    "        )  # if you use SLURM; adapt for your scheduler\n",
    "        sh.write(\"#SBATCH --output=blast_job.out\\n\\n\")\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            probe = row[\"probe\"]\n",
    "            seq = row[\"target_rc\"]  # use reverse complement for off-target binding\n",
    "\n",
    "            fasta_file = os.path.join(outdir, f\"{probe}.fasta\")\n",
    "            out_file = os.path.join(outdir, f\"{probe}_blast.txt\")\n",
    "\n",
    "            # write fasta\n",
    "            with open(fasta_file, \"w\") as f:\n",
    "                f.write(f\">{probe}\\n{seq}\\n\")\n",
    "\n",
    "            # blast command\n",
    "            cmd = (\n",
    "                f\"blastn -query {fasta_file} \"\n",
    "                f\"-db {db_path} \"\n",
    "                f\"-outfmt '10 std qseq sseq' \"\n",
    "                f\"-out {out_file} \"\n",
    "                f\"-word_size {word_size} -strand {strand}\"\n",
    "            )\n",
    "            sh.write(cmd + \"\\n\")\n",
    "\n",
    "    # Make the script executable\n",
    "    os.chmod(script_path, 0o755)\n",
    "    return script_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a3d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = write_blast_script(\n",
    "    merged_sequences,\n",
    "    db_path=blast_db_file,\n",
    "    outdir=\"blast_results\",\n",
    "    script_name=\"run_blast.sh\",\n",
    "    word_size=7,\n",
    "    strand=\"plus\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e7b01c",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b57ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sequences[\"pident\"] = np.ones(len(merged_sequences))\n",
    "merged_sequences[\"length\"] = np.ones(len(merged_sequences))\n",
    "merged_sequences[\"gap\"] = np.ones(len(merged_sequences))\n",
    "\n",
    "for idx, row in merged_sequences.iterrows():\n",
    "    results = pd.read_csv(\"blast_results/\" + row.probe + \"_blast.txt\")\n",
    "    first_hit = results.iloc[0]\n",
    "    merged_sequences.loc[idx, \"pident\"] = first_hit[2]\n",
    "    merged_sequences.loc[idx, \"length\"] = first_hit[3]\n",
    "    merged_sequences.loc[idx, \"gap\"] = first_hit[5]\n",
    "merged_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a82436",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(merged_sequences[\"gap\"], bins=20)\n",
    "plt.title(\"Gap length between primer and probe\")\n",
    "plt.xlabel(\"Gap length (bp)\")\n",
    "plt.ylabel(\"Number of probes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323522a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(merged_sequences[\"length\"], bins=20)\n",
    "plt.title(\"Length of first hit\")\n",
    "plt.xlabel(\"length (bp)\")\n",
    "plt.ylabel(\"Number of probes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6748d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(merged_sequences[\"length\"], merged_sequences[\"gap\"])\n",
    "plt.title(\"Length vs Gap of first hit\")\n",
    "plt.xlabel(\"Length (bp)\")\n",
    "plt.ylabel(\"Gap length (bp)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7aa479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c4f3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91e34957",
   "metadata": {},
   "source": [
    "## Running the first trial panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da8230",
   "metadata": {},
   "source": [
    "Running the spapros_10K_box_sparseness panel, dropping Vip and Sst because they go to hybridization, and dropping Npsr1 and Rrad because they’re very, very bad. We include the four closest markers in the list, which are Cxcl14, Cort, Pld5, Htr2c. Run a new panel excluding these four directly with an entropy constraint, which is probably better, but this will help with figuring out the kinks. \n",
    "\n",
    "We'll need to build a folder with a csv for each of these genes and run the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642398f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "panelpath = \"/nemo/lab/znamenskiyp/home/shared/projects/colasa_MOs_panel/panels/spapros_10K_taylored_2021/probeset.csv\"\n",
    "panel = pd.read_csv(panelpath)\n",
    "panel = panel[panel[\"selection\"] == True].copy()\n",
    "genes = list(panel[\"Unnamed: 0\"])\n",
    "genes, len(genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ffc1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory\n",
    "outdir = \"spapros_10K_taylored_2021\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "# Write one CSV per gene (only the gene name, no header, no index)\n",
    "for g in genes:\n",
    "    filepath = os.path.join(outdir, f\"{g}.csv\")\n",
    "    pd.Series([g]).to_csv(filepath, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f0aa45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "940e4677",
   "metadata": {},
   "source": [
    "## Debugging process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b856922",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir_temp = \"/nemo/lab/znamenskiyp/scratch/spapros_10K_taylored_2021_16_ensembl/Adamts2/TempFolder20251027140913\"\n",
    "outdir = \"/nemo/lab/znamenskiyp/scratch/spapros_10K_taylored_2021_16_ensembl/Adamts2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c84e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "designpars = (\"mouse\", 20, 2, 60, 78, 20, 44)\n",
    "\n",
    "outpars = (outdir, outdir_temp)\n",
    "outpars[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8147048",
   "metadata": {},
   "outputs": [],
   "source": [
    "genefile = \"/nemo/lab/znamenskiyp/home/users/colasa/code/multi_padlock_design/spapros_10K_taylored_2021_16_ensembl/Adamts2.csv\"\n",
    "species = \"mouse\"\n",
    "\n",
    "if len(genefile):\n",
    "    genefile = checkinput.correctinput(genefile)\n",
    "    success_g, allgenes, alllinkers = checkinput.readgenefile(genefile)\n",
    "    # remove gene name duplicates\n",
    "    genes = []\n",
    "    linkers = []\n",
    "    for c, name in enumerate(allgenes):\n",
    "        if name not in genes:\n",
    "            genes.append(name)\n",
    "            linkers.append(alllinkers[c])\n",
    "    # genes = list(set(genes))\n",
    "else:\n",
    "    success_g = True\n",
    "    (\n",
    "        success_f,\n",
    "        seqfile,\n",
    "        headers,\n",
    "        sequences,\n",
    "        headers_wpos,\n",
    "        basepos,\n",
    "    ) = checkinput.readseqfile()\n",
    "    print(success_f)\n",
    "    toavoid = [\":\", \"/\", \"\\\\\", \"[\", \"]\", \"?\", '\"', \" \", \"<\", \">\"]\n",
    "    genes = []\n",
    "    linkers = []\n",
    "    variants = []\n",
    "    print(f\"Headers: {headers}\")\n",
    "    for header in headers:\n",
    "        for i in toavoid:\n",
    "            header = header.replace(i, \"\")\n",
    "        header = header.replace(\",\", \"-\")\n",
    "        genes.append(header)\n",
    "    # Convert to Ensembl gene names\n",
    "    # Load the conversion table\n",
    "    conversion_table = pd.read_csv(\n",
    "        \"/nemo/lab/znamenskiyp/home/users/becalia/code/multi_padlock_design/data/olfr_consensus_cds_3utr_annotations.csv\"\n",
    "    )\n",
    "\n",
    "    # Build symbol(lowercased) -> first-seen alias mapping (preserves original behavior)\n",
    "    symbol_to_alias = {}\n",
    "    for _, row in conversion_table.iterrows():\n",
    "        symbol = str(row[\"gene_name\"])\n",
    "        aliases = [a.strip() for a in str(row[\"gene_symbol\"]).split(\",\") if a.strip()]\n",
    "        key = symbol.lower()\n",
    "        if aliases and key not in symbol_to_alias:\n",
    "            symbol_to_alias[key] = aliases[0]\n",
    "\n",
    "    def to_alias(name: str) -> str:\n",
    "        \"\"\"Return the alias for a given gene name, or the original if none found.\"\"\"\n",
    "        return symbol_to_alias.get(name.lower(), name)\n",
    "\n",
    "    genes = [to_alias(g) for g in genes]\n",
    "    print(f\"Genes: {genes}\")\n",
    "\n",
    "    # --- Retrieve sequences / hits ---\n",
    "    hits = retrieveseq.querygenes(genes, species)\n",
    "    print(f\"Hits: {hits}\")\n",
    "\n",
    "    retrieveseq.loaddb(species)\n",
    "    Headers = retrieveseq.Headers\n",
    "\n",
    "    # --- Collect variants and multi-hit headers for MSA ---\n",
    "    variants = []\n",
    "    headersMSA = []\n",
    "\n",
    "    def variant_id_from_header(h: str) -> str:\n",
    "        # Strip leading '>' (if present) and the suffix after the first dot\n",
    "        return h[1:].split(\".\", 1)[0]\n",
    "\n",
    "    for hit in hits:\n",
    "        if len(hit) == 1:\n",
    "            # FIX: append the whole variant id (not characters of the string)\n",
    "            variants.append(variant_id_from_header(Headers[hit[0]]))\n",
    "        elif len(hit) > 1:\n",
    "            print(\"Checking hits\")\n",
    "            tempheaders = [Headers[i] for i in hit]\n",
    "            headersMSA.append(tempheaders)\n",
    "            variants.extend(variant_id_from_header(h) for h in tempheaders)\n",
    "        # if len(hit) == 0: nothing to do\n",
    "\n",
    "    # One entry per input header, as in original\n",
    "    linkers = [[] for _ in headers]\n",
    "    variants_matching_sequence = variants\n",
    "\n",
    "    print(f\"Variants: {variants}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8714826",
   "metadata": {},
   "outputs": [],
   "source": [
    "siteChopped = [\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    5,\n",
    "    6,\n",
    "    7,\n",
    "    8,\n",
    "    9,\n",
    "    10,\n",
    "    11,\n",
    "    12,\n",
    "    13,\n",
    "    14,\n",
    "    15,\n",
    "    16,\n",
    "    17,\n",
    "    18,\n",
    "    19,\n",
    "    20,\n",
    "    21,\n",
    "    22,\n",
    "    23,\n",
    "    24,\n",
    "    25,\n",
    "    26,\n",
    "    27,\n",
    "    28,\n",
    "    29,\n",
    "    30,\n",
    "    31,\n",
    "    32,\n",
    "    33,\n",
    "    34,\n",
    "    35,\n",
    "    36,\n",
    "    37,\n",
    "    38,\n",
    "    39,\n",
    "    40,\n",
    "    41,\n",
    "    42,\n",
    "    43,\n",
    "    44,\n",
    "    45,\n",
    "    46,\n",
    "    47,\n",
    "    48,\n",
    "    49,\n",
    "    50,\n",
    "    51,\n",
    "    52,\n",
    "    53,\n",
    "    94,\n",
    "    95,\n",
    "    96,\n",
    "    97,\n",
    "    98,\n",
    "    99,\n",
    "    100,\n",
    "    101,\n",
    "    102,\n",
    "    103,\n",
    "    104,\n",
    "    105,\n",
    "    106,\n",
    "    107,\n",
    "    108,\n",
    "    109,\n",
    "    110,\n",
    "    111,\n",
    "    112,\n",
    "    113,\n",
    "    114,\n",
    "    115,\n",
    "    116,\n",
    "    117,\n",
    "    118,\n",
    "    119,\n",
    "    120,\n",
    "    121,\n",
    "    122,\n",
    "    123,\n",
    "    124,\n",
    "    125,\n",
    "    126,\n",
    "    127,\n",
    "    128,\n",
    "    129,\n",
    "    130,\n",
    "    131,\n",
    "    132,\n",
    "    133,\n",
    "    134,\n",
    "    135,\n",
    "    136,\n",
    "    137,\n",
    "    138,\n",
    "    139,\n",
    "    140,\n",
    "    141,\n",
    "    142,\n",
    "    143,\n",
    "    144,\n",
    "    145,\n",
    "    146,\n",
    "    147,\n",
    "    148,\n",
    "    149,\n",
    "    150,\n",
    "    151,\n",
    "    152,\n",
    "    153,\n",
    "    154,\n",
    "    155,\n",
    "    156,\n",
    "    157,\n",
    "    158,\n",
    "    159,\n",
    "    160,\n",
    "    161,\n",
    "    162,\n",
    "    163,\n",
    "    164,\n",
    "    165,\n",
    "    166,\n",
    "    167,\n",
    "    168,\n",
    "    169,\n",
    "    170,\n",
    "    171,\n",
    "    172,\n",
    "    173,\n",
    "    174,\n",
    "    175,\n",
    "    176,\n",
    "    177,\n",
    "    178,\n",
    "    179,\n",
    "    180,\n",
    "    181,\n",
    "    182,\n",
    "    183,\n",
    "    195,\n",
    "    196,\n",
    "    197,\n",
    "    204,\n",
    "    290,\n",
    "    291,\n",
    "    292,\n",
    "    293,\n",
    "    294,\n",
    "    295,\n",
    "    296,\n",
    "    297,\n",
    "    298,\n",
    "    299,\n",
    "    300,\n",
    "    301,\n",
    "    302,\n",
    "    303,\n",
    "    304,\n",
    "    305,\n",
    "    306,\n",
    "    307,\n",
    "    308,\n",
    "    309,\n",
    "    310,\n",
    "    311,\n",
    "    312,\n",
    "    313,\n",
    "    314,\n",
    "    315,\n",
    "    316,\n",
    "    317,\n",
    "    318,\n",
    "    319,\n",
    "    320,\n",
    "    321,\n",
    "    322,\n",
    "    323,\n",
    "    324,\n",
    "    325,\n",
    "    326,\n",
    "    327,\n",
    "    328,\n",
    "    329,\n",
    "    330,\n",
    "    331,\n",
    "    332,\n",
    "    333,\n",
    "    338,\n",
    "    339,\n",
    "    340,\n",
    "    341,\n",
    "    342,\n",
    "    343,\n",
    "    344,\n",
    "    345,\n",
    "    346,\n",
    "    347,\n",
    "    348,\n",
    "    349,\n",
    "    350,\n",
    "    351,\n",
    "    352,\n",
    "    353,\n",
    "    354,\n",
    "    355,\n",
    "    356,\n",
    "    357,\n",
    "    358,\n",
    "    359,\n",
    "    360,\n",
    "    361,\n",
    "    362,\n",
    "    363,\n",
    "    364,\n",
    "    365,\n",
    "    366,\n",
    "    367,\n",
    "    368,\n",
    "    369,\n",
    "    370,\n",
    "    371,\n",
    "    372,\n",
    "    373,\n",
    "    374,\n",
    "    375,\n",
    "    376,\n",
    "    377,\n",
    "    378,\n",
    "    379,\n",
    "    380,\n",
    "    381,\n",
    "    382,\n",
    "    383,\n",
    "    384,\n",
    "    385,\n",
    "    386,\n",
    "    387,\n",
    "    388,\n",
    "    389,\n",
    "    390,\n",
    "    391,\n",
    "    392,\n",
    "    393,\n",
    "    394,\n",
    "    395,\n",
    "    396,\n",
    "    397,\n",
    "    398,\n",
    "    399,\n",
    "    400,\n",
    "    401,\n",
    "    402,\n",
    "    403,\n",
    "    404,\n",
    "    405,\n",
    "    406,\n",
    "    407,\n",
    "    408,\n",
    "    409,\n",
    "    410,\n",
    "    411,\n",
    "    412,\n",
    "    413,\n",
    "    414,\n",
    "    415,\n",
    "    416,\n",
    "    417,\n",
    "    418,\n",
    "    419,\n",
    "    420,\n",
    "    421,\n",
    "    422,\n",
    "    423,\n",
    "    424,\n",
    "    425,\n",
    "    426,\n",
    "    427,\n",
    "    428,\n",
    "    429,\n",
    "    430,\n",
    "    431,\n",
    "    432,\n",
    "    433,\n",
    "    434,\n",
    "    435,\n",
    "    436,\n",
    "    437,\n",
    "    438,\n",
    "    439,\n",
    "    440,\n",
    "    441,\n",
    "    442,\n",
    "    443,\n",
    "    444,\n",
    "    445,\n",
    "    446,\n",
    "    447,\n",
    "    448,\n",
    "    449,\n",
    "    450,\n",
    "    451,\n",
    "    452,\n",
    "    453,\n",
    "    454,\n",
    "    455,\n",
    "    456,\n",
    "    457,\n",
    "    458,\n",
    "    459,\n",
    "    460,\n",
    "    461,\n",
    "    462,\n",
    "    463,\n",
    "    464,\n",
    "    465,\n",
    "    466,\n",
    "    467,\n",
    "    468,\n",
    "    469,\n",
    "    470,\n",
    "    471,\n",
    "    472,\n",
    "    473,\n",
    "    474,\n",
    "    475,\n",
    "    476,\n",
    "    477,\n",
    "    478,\n",
    "    479,\n",
    "    480,\n",
    "    481,\n",
    "    482,\n",
    "    483,\n",
    "    484,\n",
    "    485,\n",
    "    486,\n",
    "    487,\n",
    "    488,\n",
    "    489,\n",
    "    490,\n",
    "    491,\n",
    "    492,\n",
    "    493,\n",
    "    494,\n",
    "    495,\n",
    "    496,\n",
    "    497,\n",
    "    498,\n",
    "    499,\n",
    "    500,\n",
    "    501,\n",
    "    502,\n",
    "    503,\n",
    "    504,\n",
    "    505,\n",
    "    506,\n",
    "    507,\n",
    "    508,\n",
    "    509,\n",
    "    510,\n",
    "    511,\n",
    "    512,\n",
    "    513,\n",
    "    514,\n",
    "    515,\n",
    "    516,\n",
    "    517,\n",
    "    518,\n",
    "    519,\n",
    "    520,\n",
    "    521,\n",
    "    522,\n",
    "    523,\n",
    "    524,\n",
    "    525,\n",
    "    526,\n",
    "    527,\n",
    "    528,\n",
    "    529,\n",
    "    530,\n",
    "    531,\n",
    "    532,\n",
    "    533,\n",
    "    534,\n",
    "    535,\n",
    "    536,\n",
    "    537,\n",
    "    538,\n",
    "    539,\n",
    "    540,\n",
    "    541,\n",
    "    542,\n",
    "    543,\n",
    "    544,\n",
    "    545,\n",
    "    546,\n",
    "    547,\n",
    "    548,\n",
    "    549,\n",
    "    550,\n",
    "    551,\n",
    "    552,\n",
    "    553,\n",
    "    554,\n",
    "    555,\n",
    "    556,\n",
    "    557,\n",
    "    558,\n",
    "    559,\n",
    "    560,\n",
    "    561,\n",
    "    562,\n",
    "    563,\n",
    "    564,\n",
    "    565,\n",
    "    566,\n",
    "    567,\n",
    "    568,\n",
    "    569,\n",
    "    570,\n",
    "    571,\n",
    "    572,\n",
    "    573,\n",
    "    574,\n",
    "    575,\n",
    "    576,\n",
    "    577,\n",
    "    578,\n",
    "    579,\n",
    "    580,\n",
    "    581,\n",
    "    582,\n",
    "    583,\n",
    "    584,\n",
    "    585,\n",
    "    586,\n",
    "    587,\n",
    "    588,\n",
    "    589,\n",
    "    590,\n",
    "    591,\n",
    "    592,\n",
    "    593,\n",
    "    594,\n",
    "    595,\n",
    "    596,\n",
    "    597,\n",
    "    598,\n",
    "    599,\n",
    "    600,\n",
    "    601,\n",
    "    602,\n",
    "    603,\n",
    "    604,\n",
    "    605,\n",
    "    606,\n",
    "    607,\n",
    "    608,\n",
    "    609,\n",
    "    610,\n",
    "    611,\n",
    "    612,\n",
    "    613,\n",
    "    614,\n",
    "    615,\n",
    "    616,\n",
    "    617,\n",
    "    618,\n",
    "    619,\n",
    "    620,\n",
    "    621,\n",
    "    622,\n",
    "    623,\n",
    "    624,\n",
    "    625,\n",
    "    626,\n",
    "    627,\n",
    "    628,\n",
    "    629,\n",
    "    630,\n",
    "    631,\n",
    "    632,\n",
    "    633,\n",
    "    634,\n",
    "    635,\n",
    "    636,\n",
    "    637,\n",
    "    638,\n",
    "    639,\n",
    "    640,\n",
    "    641,\n",
    "    642,\n",
    "    643,\n",
    "    644,\n",
    "    645,\n",
    "    646,\n",
    "    647,\n",
    "    648,\n",
    "    649,\n",
    "    650,\n",
    "    651,\n",
    "    652,\n",
    "    653,\n",
    "    654,\n",
    "    655,\n",
    "    656,\n",
    "    657,\n",
    "    658,\n",
    "    659,\n",
    "    660,\n",
    "    661,\n",
    "    662,\n",
    "    663,\n",
    "    664,\n",
    "    665,\n",
    "    666,\n",
    "    667,\n",
    "    668,\n",
    "    669,\n",
    "    670,\n",
    "    671,\n",
    "    672,\n",
    "    673,\n",
    "    674,\n",
    "    675,\n",
    "    676,\n",
    "    677,\n",
    "    678,\n",
    "    679,\n",
    "    680,\n",
    "    681,\n",
    "    682,\n",
    "    683,\n",
    "    684,\n",
    "    685,\n",
    "    686,\n",
    "    687,\n",
    "    688,\n",
    "    689,\n",
    "    690,\n",
    "    691,\n",
    "    692,\n",
    "    693,\n",
    "    694,\n",
    "    695,\n",
    "    696,\n",
    "    697,\n",
    "    698,\n",
    "    699,\n",
    "    700,\n",
    "    701,\n",
    "    702,\n",
    "    703,\n",
    "    704,\n",
    "    705,\n",
    "    706,\n",
    "    707,\n",
    "    708,\n",
    "    709,\n",
    "    710,\n",
    "    711,\n",
    "    712,\n",
    "    713,\n",
    "    714,\n",
    "    715,\n",
    "    716,\n",
    "    717,\n",
    "    718,\n",
    "    719,\n",
    "    720,\n",
    "    721,\n",
    "    722,\n",
    "    723,\n",
    "    724,\n",
    "    725,\n",
    "    726,\n",
    "    727,\n",
    "    728,\n",
    "    729,\n",
    "    730,\n",
    "    731,\n",
    "    732,\n",
    "    733,\n",
    "    734,\n",
    "    735,\n",
    "    736,\n",
    "    737,\n",
    "    738,\n",
    "    739,\n",
    "    740,\n",
    "    741,\n",
    "    742,\n",
    "    743,\n",
    "    744,\n",
    "    745,\n",
    "    746,\n",
    "    747,\n",
    "    748,\n",
    "    749,\n",
    "    750,\n",
    "    751,\n",
    "    752,\n",
    "    753,\n",
    "    754,\n",
    "    755,\n",
    "    756,\n",
    "    757,\n",
    "    758,\n",
    "    759,\n",
    "    760,\n",
    "    761,\n",
    "    762,\n",
    "    763,\n",
    "    764,\n",
    "    765,\n",
    "    766,\n",
    "    767,\n",
    "    768,\n",
    "    769,\n",
    "    770,\n",
    "    771,\n",
    "    772,\n",
    "    773,\n",
    "    774,\n",
    "    775,\n",
    "    776,\n",
    "    777,\n",
    "    778,\n",
    "    779,\n",
    "    780,\n",
    "    781,\n",
    "    782,\n",
    "    783,\n",
    "    784,\n",
    "    785,\n",
    "    786,\n",
    "    787,\n",
    "    788,\n",
    "    789,\n",
    "    790,\n",
    "    791,\n",
    "    792,\n",
    "    793,\n",
    "    794,\n",
    "    795,\n",
    "    796,\n",
    "    797,\n",
    "    798,\n",
    "    799,\n",
    "    800,\n",
    "    801,\n",
    "    802,\n",
    "    803,\n",
    "    804,\n",
    "    805,\n",
    "    806,\n",
    "    807,\n",
    "    808,\n",
    "    809,\n",
    "    810,\n",
    "    811,\n",
    "    812,\n",
    "    813,\n",
    "    814,\n",
    "    815,\n",
    "    816,\n",
    "    817,\n",
    "    818,\n",
    "    819,\n",
    "    820,\n",
    "    821,\n",
    "    822,\n",
    "    823,\n",
    "    824,\n",
    "    825,\n",
    "    826,\n",
    "    827,\n",
    "    828,\n",
    "    829,\n",
    "    830,\n",
    "    831,\n",
    "    832,\n",
    "    833,\n",
    "    834,\n",
    "    835,\n",
    "    836,\n",
    "    837,\n",
    "    838,\n",
    "    839,\n",
    "    840,\n",
    "    841,\n",
    "    842,\n",
    "    843,\n",
    "    844,\n",
    "    845,\n",
    "    846,\n",
    "    847,\n",
    "    848,\n",
    "    849,\n",
    "    850,\n",
    "    851,\n",
    "    852,\n",
    "    853,\n",
    "    854,\n",
    "    855,\n",
    "    856,\n",
    "    857,\n",
    "    858,\n",
    "    859,\n",
    "    860,\n",
    "    861,\n",
    "    862,\n",
    "    863,\n",
    "    864,\n",
    "    865,\n",
    "    866,\n",
    "    867,\n",
    "    868,\n",
    "    869,\n",
    "    870,\n",
    "    871,\n",
    "    872,\n",
    "    873,\n",
    "    874,\n",
    "    875,\n",
    "    876,\n",
    "    877,\n",
    "    878,\n",
    "    879,\n",
    "    880,\n",
    "    881,\n",
    "    882,\n",
    "    883,\n",
    "    884,\n",
    "    885,\n",
    "    886,\n",
    "    887,\n",
    "    888,\n",
    "    889,\n",
    "    890,\n",
    "    891,\n",
    "    892,\n",
    "    893,\n",
    "    894,\n",
    "    895,\n",
    "    896,\n",
    "    897,\n",
    "    898,\n",
    "    899,\n",
    "    900,\n",
    "    901,\n",
    "    902,\n",
    "    903,\n",
    "    904,\n",
    "    905,\n",
    "    906,\n",
    "    907,\n",
    "    908,\n",
    "    909,\n",
    "    910,\n",
    "    911,\n",
    "    912,\n",
    "    913,\n",
    "    914,\n",
    "    915,\n",
    "    916,\n",
    "    917,\n",
    "    918,\n",
    "    919,\n",
    "    920,\n",
    "    921,\n",
    "    922,\n",
    "    923,\n",
    "    924,\n",
    "    925,\n",
    "    926,\n",
    "    927,\n",
    "    928,\n",
    "    929,\n",
    "    930,\n",
    "    931,\n",
    "    932,\n",
    "    933,\n",
    "    934,\n",
    "    935,\n",
    "    936,\n",
    "    937,\n",
    "    938,\n",
    "    939,\n",
    "    940,\n",
    "    941,\n",
    "    942,\n",
    "    943,\n",
    "    944,\n",
    "    945,\n",
    "    946,\n",
    "    947,\n",
    "    948,\n",
    "    949,\n",
    "    950,\n",
    "    951,\n",
    "    952,\n",
    "    953,\n",
    "    954,\n",
    "    955,\n",
    "    956,\n",
    "    957,\n",
    "    958,\n",
    "    959,\n",
    "    960,\n",
    "    961,\n",
    "    962,\n",
    "    963,\n",
    "    964,\n",
    "    965,\n",
    "    966,\n",
    "    967,\n",
    "    968,\n",
    "    969,\n",
    "    970,\n",
    "    971,\n",
    "    972,\n",
    "    973,\n",
    "    974,\n",
    "    975,\n",
    "    976,\n",
    "    977,\n",
    "    978,\n",
    "    979,\n",
    "    980,\n",
    "    981,\n",
    "    982,\n",
    "    983,\n",
    "    984,\n",
    "    985,\n",
    "    986,\n",
    "    987,\n",
    "    988,\n",
    "    989,\n",
    "    990,\n",
    "    991,\n",
    "    992,\n",
    "    993,\n",
    "    994,\n",
    "    995,\n",
    "    996,\n",
    "    997,\n",
    "    998,\n",
    "    999,\n",
    "    1000,\n",
    "    1001,\n",
    "    1002,\n",
    "    1003,\n",
    "    1004,\n",
    "    1005,\n",
    "    1006,\n",
    "    1007,\n",
    "    1008,\n",
    "    1009,\n",
    "    1010,\n",
    "    1011,\n",
    "    1012,\n",
    "    1013,\n",
    "    1014,\n",
    "    1015,\n",
    "    1016,\n",
    "    1017,\n",
    "    1018,\n",
    "    1019,\n",
    "    1020,\n",
    "    1021,\n",
    "    1022,\n",
    "    1023,\n",
    "    1024,\n",
    "    1025,\n",
    "    1026,\n",
    "    1027,\n",
    "    1028,\n",
    "    1029,\n",
    "    1030,\n",
    "    1031,\n",
    "    1032,\n",
    "    1033,\n",
    "    1034,\n",
    "    1035,\n",
    "    1036,\n",
    "    1037,\n",
    "    1038,\n",
    "    1039,\n",
    "    1040,\n",
    "    1041,\n",
    "    1042,\n",
    "    1043,\n",
    "    1044,\n",
    "    1045,\n",
    "    1046,\n",
    "    1047,\n",
    "    1048,\n",
    "    1049,\n",
    "    1050,\n",
    "    1051,\n",
    "    1052,\n",
    "    1053,\n",
    "    1054,\n",
    "    1055,\n",
    "    1056,\n",
    "    1057,\n",
    "    1058,\n",
    "    1059,\n",
    "    1060,\n",
    "    1061,\n",
    "    1062,\n",
    "    1063,\n",
    "    1064,\n",
    "    1065,\n",
    "    1066,\n",
    "    1067,\n",
    "    1068,\n",
    "    1069,\n",
    "    1070,\n",
    "    1071,\n",
    "    1072,\n",
    "    1073,\n",
    "    1074,\n",
    "    1075,\n",
    "    1076,\n",
    "    1077,\n",
    "    1078,\n",
    "    1079,\n",
    "    1080,\n",
    "    1081,\n",
    "    1082,\n",
    "    1083,\n",
    "    1084,\n",
    "    1085,\n",
    "    1086,\n",
    "    1087,\n",
    "    1088,\n",
    "    1089,\n",
    "    1090,\n",
    "    1091,\n",
    "    1092,\n",
    "    1093,\n",
    "    1094,\n",
    "    1095,\n",
    "    1096,\n",
    "    1097,\n",
    "    1098,\n",
    "    1099,\n",
    "    1100,\n",
    "    1101,\n",
    "    1102,\n",
    "    1103,\n",
    "    1104,\n",
    "    1105,\n",
    "    1106,\n",
    "    1107,\n",
    "    1108,\n",
    "    1109,\n",
    "    1110,\n",
    "    1111,\n",
    "    1112,\n",
    "    1113,\n",
    "    1114,\n",
    "    1115,\n",
    "    1116,\n",
    "    1117,\n",
    "    1118,\n",
    "    1119,\n",
    "    1120,\n",
    "    1121,\n",
    "    1122,\n",
    "    1123,\n",
    "    1124,\n",
    "    1125,\n",
    "    1126,\n",
    "    1127,\n",
    "    1128,\n",
    "    1129,\n",
    "    1130,\n",
    "    1131,\n",
    "    1132,\n",
    "    1133,\n",
    "    1134,\n",
    "    1135,\n",
    "    1136,\n",
    "    1137,\n",
    "    1138,\n",
    "    1139,\n",
    "    1140,\n",
    "    1141,\n",
    "    1142,\n",
    "    1143,\n",
    "    1144,\n",
    "    1145,\n",
    "    1146,\n",
    "    1147,\n",
    "    1148,\n",
    "    1149,\n",
    "    1150,\n",
    "    1151,\n",
    "    1152,\n",
    "    1153,\n",
    "    1154,\n",
    "    1155,\n",
    "    1156,\n",
    "    1157,\n",
    "    1158,\n",
    "    1159,\n",
    "    1160,\n",
    "    1161,\n",
    "    1162,\n",
    "    1163,\n",
    "    1164,\n",
    "    1165,\n",
    "    1166,\n",
    "    1167,\n",
    "    1168,\n",
    "    1169,\n",
    "    1170,\n",
    "    1171,\n",
    "    1172,\n",
    "    1173,\n",
    "    1174,\n",
    "    1175,\n",
    "    1176,\n",
    "    1177,\n",
    "    1178,\n",
    "    1179,\n",
    "    1180,\n",
    "    1181,\n",
    "    1182,\n",
    "    1183,\n",
    "    1184,\n",
    "    1185,\n",
    "    1186,\n",
    "    1187,\n",
    "    1188,\n",
    "    1189,\n",
    "    1190,\n",
    "    1191,\n",
    "    1192,\n",
    "    1193,\n",
    "    1194,\n",
    "    1195,\n",
    "    1196,\n",
    "    1197,\n",
    "    1198,\n",
    "    1199,\n",
    "    1200,\n",
    "    1201,\n",
    "    1202,\n",
    "    1203,\n",
    "    1204,\n",
    "    1205,\n",
    "    1206,\n",
    "    1207,\n",
    "    1208,\n",
    "    1209,\n",
    "    1210,\n",
    "    1211,\n",
    "    1212,\n",
    "    1213,\n",
    "    1214,\n",
    "    1215,\n",
    "    1216,\n",
    "    1217,\n",
    "    1218,\n",
    "    1219,\n",
    "    1220,\n",
    "    1296,\n",
    "    1297,\n",
    "    1298,\n",
    "    1299,\n",
    "    1300,\n",
    "    1301,\n",
    "    1302,\n",
    "    1303,\n",
    "    1304,\n",
    "    1305,\n",
    "    1306,\n",
    "    1307,\n",
    "    1308,\n",
    "    1309,\n",
    "    1310,\n",
    "    1311,\n",
    "    1312,\n",
    "    1313,\n",
    "    1314,\n",
    "    1315,\n",
    "    1316,\n",
    "    1317,\n",
    "    1318,\n",
    "    1319,\n",
    "    1320,\n",
    "    1321,\n",
    "    1322,\n",
    "    1323,\n",
    "    1324,\n",
    "    1325,\n",
    "    1326,\n",
    "    1327,\n",
    "    1328,\n",
    "    1329,\n",
    "    1330,\n",
    "    1331,\n",
    "    1332,\n",
    "    1333,\n",
    "    1334,\n",
    "    1335,\n",
    "    1336,\n",
    "    1337,\n",
    "    1338,\n",
    "    1339,\n",
    "    1340,\n",
    "    1341,\n",
    "    1342,\n",
    "    1343,\n",
    "    1344,\n",
    "    1345,\n",
    "    1346,\n",
    "    1347,\n",
    "    1348,\n",
    "    1349,\n",
    "    1350,\n",
    "    1351,\n",
    "    1352,\n",
    "    1353,\n",
    "    1354,\n",
    "    1355,\n",
    "    1356,\n",
    "    1357,\n",
    "    1358,\n",
    "    1359,\n",
    "    1360,\n",
    "    1361,\n",
    "    1362,\n",
    "    1363,\n",
    "    1364,\n",
    "    1365,\n",
    "    1366,\n",
    "    1367,\n",
    "    1368,\n",
    "    1369,\n",
    "    1370,\n",
    "    1371,\n",
    "    1372,\n",
    "    1373,\n",
    "    1374,\n",
    "    1375,\n",
    "    1417,\n",
    "    1418,\n",
    "    1419,\n",
    "    1420,\n",
    "    1421,\n",
    "    1422,\n",
    "    1423,\n",
    "    1424,\n",
    "    1425,\n",
    "    1426,\n",
    "    1427,\n",
    "    1428,\n",
    "    1429,\n",
    "    1430,\n",
    "    1431,\n",
    "    1432,\n",
    "    1433,\n",
    "    1434,\n",
    "    1435,\n",
    "    1436,\n",
    "    1437,\n",
    "    1438,\n",
    "    1439,\n",
    "    1440,\n",
    "    1441,\n",
    "    1442,\n",
    "    1443,\n",
    "    1444,\n",
    "    1445,\n",
    "    1446,\n",
    "    1447,\n",
    "    1448,\n",
    "    1449,\n",
    "    1450,\n",
    "    1451,\n",
    "    1452,\n",
    "    1453,\n",
    "    1454,\n",
    "    1455,\n",
    "    1456,\n",
    "    1457,\n",
    "    1458,\n",
    "    1459,\n",
    "    1460,\n",
    "    1461,\n",
    "    1462,\n",
    "    1463,\n",
    "    1464,\n",
    "    1465,\n",
    "    1466,\n",
    "    1467,\n",
    "    1468,\n",
    "    1469,\n",
    "    1470,\n",
    "    1471,\n",
    "    1472,\n",
    "    1473,\n",
    "    1474,\n",
    "    1475,\n",
    "    1476,\n",
    "    1477,\n",
    "    1478,\n",
    "    1479,\n",
    "    1480,\n",
    "    1481,\n",
    "    1482,\n",
    "    1483,\n",
    "    1484,\n",
    "    1485,\n",
    "    1486,\n",
    "    1487,\n",
    "    1488,\n",
    "    1489,\n",
    "    1490,\n",
    "    1491,\n",
    "    1492,\n",
    "    1493,\n",
    "    1494,\n",
    "    1495,\n",
    "    1496,\n",
    "    1497,\n",
    "    1498,\n",
    "    1499,\n",
    "    1500,\n",
    "    1501,\n",
    "    1502,\n",
    "    1503,\n",
    "    1504,\n",
    "    1505,\n",
    "    1506,\n",
    "    1507,\n",
    "    1508,\n",
    "    1509,\n",
    "    1510,\n",
    "    1511,\n",
    "    1512,\n",
    "    1513,\n",
    "    1514,\n",
    "    1515,\n",
    "    1516,\n",
    "    1517,\n",
    "    1518,\n",
    "    1519,\n",
    "    1520,\n",
    "    1521,\n",
    "    1522,\n",
    "    1523,\n",
    "    1524,\n",
    "    1525,\n",
    "    1526,\n",
    "    1527,\n",
    "    1528,\n",
    "    1529,\n",
    "    1530,\n",
    "    1531,\n",
    "    1532,\n",
    "    1533,\n",
    "    1534,\n",
    "    1535,\n",
    "    1536,\n",
    "    1537,\n",
    "    1538,\n",
    "    1539,\n",
    "    1540,\n",
    "    1541,\n",
    "    1542,\n",
    "    1543,\n",
    "    1544,\n",
    "    1545,\n",
    "    1546,\n",
    "    1547,\n",
    "    1548,\n",
    "    1549,\n",
    "    1550,\n",
    "    1551,\n",
    "    1552,\n",
    "    1553,\n",
    "    1554,\n",
    "    1555,\n",
    "    1556,\n",
    "    1557,\n",
    "    1558,\n",
    "    1559,\n",
    "    1560,\n",
    "    1561,\n",
    "    1562,\n",
    "    1563,\n",
    "    1564,\n",
    "    1565,\n",
    "    1566,\n",
    "    1567,\n",
    "    1568,\n",
    "    1569,\n",
    "    1570,\n",
    "    1571,\n",
    "    1572,\n",
    "    1573,\n",
    "    1574,\n",
    "    1575,\n",
    "    1576,\n",
    "    1577,\n",
    "    1578,\n",
    "    1579,\n",
    "    1580,\n",
    "    1581,\n",
    "    1582,\n",
    "    1583,\n",
    "    1584,\n",
    "    1585,\n",
    "    1586,\n",
    "    1587,\n",
    "    1588,\n",
    "    1589,\n",
    "    1590,\n",
    "    1591,\n",
    "    1592,\n",
    "    1593,\n",
    "    1594,\n",
    "    1595,\n",
    "    1596,\n",
    "    1597,\n",
    "    1598,\n",
    "    1599,\n",
    "    1600,\n",
    "    1601,\n",
    "    1602,\n",
    "    1603,\n",
    "    1604,\n",
    "    1605,\n",
    "    1606,\n",
    "    1607,\n",
    "    1608,\n",
    "    1609,\n",
    "    1610,\n",
    "    1611,\n",
    "    1612,\n",
    "    1613,\n",
    "    1614,\n",
    "    1615,\n",
    "    1616,\n",
    "    1617,\n",
    "    1618,\n",
    "    1619,\n",
    "    1620,\n",
    "    1621,\n",
    "    1622,\n",
    "    1623,\n",
    "    1624,\n",
    "    1665,\n",
    "    1666,\n",
    "    1667,\n",
    "    1668,\n",
    "    1669,\n",
    "    1670,\n",
    "    1671,\n",
    "    1672,\n",
    "    1673,\n",
    "    1674,\n",
    "    1675,\n",
    "    1676,\n",
    "    1677,\n",
    "    1678,\n",
    "    1679,\n",
    "    1680,\n",
    "    1681,\n",
    "    1682,\n",
    "    1683,\n",
    "    1684,\n",
    "    1685,\n",
    "    1686,\n",
    "    1687,\n",
    "    1688,\n",
    "    1689,\n",
    "    1690,\n",
    "    1691,\n",
    "    1692,\n",
    "    1693,\n",
    "    1694,\n",
    "    1695,\n",
    "    1696,\n",
    "    1697,\n",
    "    1698,\n",
    "    1699,\n",
    "    1700,\n",
    "    1701,\n",
    "    1702,\n",
    "    1703,\n",
    "    1704,\n",
    "    1705,\n",
    "    1706,\n",
    "    1707,\n",
    "    1708,\n",
    "    1709,\n",
    "    1710,\n",
    "    1711,\n",
    "    1712,\n",
    "    1713,\n",
    "    1714,\n",
    "    1715,\n",
    "    1716,\n",
    "    1717,\n",
    "    1718,\n",
    "    1719,\n",
    "    1720,\n",
    "    1721,\n",
    "    1722,\n",
    "    1723,\n",
    "    1724,\n",
    "    1725,\n",
    "    1726,\n",
    "    1727,\n",
    "    1728,\n",
    "    1729,\n",
    "    1730,\n",
    "    1731,\n",
    "    1732,\n",
    "    1733,\n",
    "    1734,\n",
    "    1735,\n",
    "    1736,\n",
    "    1737,\n",
    "    1738,\n",
    "    1739,\n",
    "    1740,\n",
    "    1741,\n",
    "    1742,\n",
    "    1743,\n",
    "    1744,\n",
    "    1745,\n",
    "    1746,\n",
    "    1747,\n",
    "    1748,\n",
    "    1749,\n",
    "    1750,\n",
    "    1751,\n",
    "    1752,\n",
    "    1753,\n",
    "    1754,\n",
    "    1755,\n",
    "    1756,\n",
    "    1757,\n",
    "    1758,\n",
    "    1759,\n",
    "    1760,\n",
    "    1761,\n",
    "    1762,\n",
    "    1763,\n",
    "    1764,\n",
    "    1765,\n",
    "    1766,\n",
    "    1767,\n",
    "    1768,\n",
    "    1769,\n",
    "    1770,\n",
    "    1771,\n",
    "    1772,\n",
    "    1773,\n",
    "    1774,\n",
    "    1775,\n",
    "    1776,\n",
    "    1777,\n",
    "    1778,\n",
    "    1779,\n",
    "    1780,\n",
    "    1781,\n",
    "    1782,\n",
    "    1783,\n",
    "    1784,\n",
    "    1785,\n",
    "    1786,\n",
    "    1787,\n",
    "    1788,\n",
    "    1789,\n",
    "    1790,\n",
    "    1791,\n",
    "    1792,\n",
    "    1793,\n",
    "    1794,\n",
    "    1795,\n",
    "    1796,\n",
    "    1797,\n",
    "    1798,\n",
    "    1799,\n",
    "    1800,\n",
    "    1801,\n",
    "    1802,\n",
    "    1803,\n",
    "    1804,\n",
    "    1805,\n",
    "    1806,\n",
    "    1807,\n",
    "    1808,\n",
    "    1809,\n",
    "    1810,\n",
    "    1811,\n",
    "    1812,\n",
    "    1813,\n",
    "    1814,\n",
    "    1815,\n",
    "    1816,\n",
    "    1817,\n",
    "    1818,\n",
    "    1819,\n",
    "    1820,\n",
    "    1821,\n",
    "    1822,\n",
    "    1823,\n",
    "    1824,\n",
    "    1825,\n",
    "    1826,\n",
    "    1827,\n",
    "    1828,\n",
    "    1829,\n",
    "    1830,\n",
    "    1831,\n",
    "    1832,\n",
    "    1833,\n",
    "    1834,\n",
    "    1835,\n",
    "    1836,\n",
    "    1837,\n",
    "    1838,\n",
    "    1839,\n",
    "    1840,\n",
    "    1841,\n",
    "    1842,\n",
    "    1843,\n",
    "    1844,\n",
    "    1845,\n",
    "    1846,\n",
    "    1847,\n",
    "    1848,\n",
    "    1849,\n",
    "    1850,\n",
    "    1851,\n",
    "    1852,\n",
    "    1853,\n",
    "    1854,\n",
    "    1855,\n",
    "    1856,\n",
    "    1857,\n",
    "    1858,\n",
    "    1859,\n",
    "    1860,\n",
    "    1861,\n",
    "    1862,\n",
    "    1863,\n",
    "    1864,\n",
    "    1865,\n",
    "    1866,\n",
    "    1867,\n",
    "    1868,\n",
    "    1869,\n",
    "    1870,\n",
    "    1871,\n",
    "    1872,\n",
    "    1873,\n",
    "    1874,\n",
    "    1875,\n",
    "    1876,\n",
    "    1877,\n",
    "    1878,\n",
    "    1879,\n",
    "    1880,\n",
    "    1881,\n",
    "    1882,\n",
    "    1883,\n",
    "    1884,\n",
    "    1885,\n",
    "]\n",
    "plt.plot(siteChopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86859ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "blastpath = \"/nemo/lab/znamenskiyp/scratch/spapros_10K_taylored_2021_20_ensembl/Lamp5/TempFolder20251031115743/ENSMUST00000057503.7cdnachromo_target_431_blast.txt\"\n",
    "with open(blastpath, \"r\") as f:\n",
    "    for line in f:\n",
    "        if not (\"XR\" in line or \"XM\" in line):\n",
    "            columns = line.split(\",\")\n",
    "            hit = columns[1].strip()\n",
    "            scores = columns[2:]\n",
    "            print(f\"Hit: {hit}\")\n",
    "            print(f\"Scores: {scores[0]}\")\n",
    "            print(scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ab1395",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not success_f:\n",
    "    # find genes in the database\n",
    "    hits = retrieveseq.querygenes(genes, species)\n",
    "\n",
    "    # if any gene is not found in the RefSeq acronym list, write to file\n",
    "    nohit = [i for i in range(len(genes)) if len(hits[i]) == 0]\n",
    "    if len(nohit):\n",
    "        # try to convert gene acronyms using Ensembl REST API\n",
    "        print(genes[1])\n",
    "        translated_id = makesnails.get_gene_synonyms(species, genes[1])\n",
    "        print(translated_id)\n",
    "        if translated_id is not None:\n",
    "            print(f\"Converted {genes} to {translated_id}\")\n",
    "            hits = retrieveseq.querygenes(translated_id, species)\n",
    "            with open(\n",
    "                os.path.join(outdir, \"0.AcronymNotFound_\" + t + \".txt\"), \"w\"\n",
    "            ) as f:\n",
    "                f.write(\n",
    "                    \"Some gene acronyms were converted using Ensembl REST API. Please check the following list:\\n\"\n",
    "                )\n",
    "                f.write(\"%s -> %s\\n\" % (translated_id, \"Gabra1\"))\n",
    "                f.write(\"Please make sure these are the intended genes.\\n\\n\")\n",
    "        else:\n",
    "            with open(\n",
    "                os.path.join(outdir, \"0.AcronymNotFound_\" + t + \".txt\"), \"w\"\n",
    "            ) as f:\n",
    "                for i in nohit[::-1]:\n",
    "                    f.write(\"%s\\n\" % genes[i])\n",
    "                    del genes[i]  # remove genes that are not found\n",
    "                    del linkers[i]\n",
    "                    del hits[i]\n",
    "\n",
    "    # find sequences (MSA included if multiple variants)\n",
    "    headers, basepos, sequences, msa, nocommon, variants = retrieveseq.findseq(\n",
    "        genes, hits, outdir_temp\n",
    "    )\n",
    "\n",
    "    # genes that have no common sequence among all variants\n",
    "    nocommon = [c for c, i in enumerate(genes) if i in nocommon]\n",
    "    if len(nocommon):\n",
    "        with open(\n",
    "            os.path.join(outdir, \"0.NoConsensusSequence_\" + t + \".txt\"), \"w\"\n",
    "        ) as f:\n",
    "            for i in nocommon[::-1]:\n",
    "                f.write(\"%s\\n\" % genes[i])\n",
    "                del genes[i]  # remove genes that are not found\n",
    "                del linkers[i]\n",
    "                del hits[i]\n",
    "                del variants[i]\n",
    "\n",
    "    idxmsa = [c for c, i in enumerate(genes) if i in msa]\n",
    "    geneorder = [c for c, i in enumerate(genes) if i not in msa]\n",
    "    [geneorder.append(i) for i in idxmsa]\n",
    "    linkers = [linkers[i] for i in geneorder]\n",
    "    genes = [genes[i] for i in geneorder]\n",
    "    variants = [variants[i] for i in geneorder]\n",
    "\n",
    "    # replicates variants so that they match headers_wpos\n",
    "    variants_matching_sequence = []\n",
    "    for c, i in enumerate(variants):\n",
    "        if isinstance(basepos[c][0], int):\n",
    "            variants_matching_sequence.append(i)\n",
    "        else:\n",
    "            for j in range(0, len(basepos[c])):\n",
    "                variants_matching_sequence.append(i)\n",
    "\n",
    "    # write found sequences to output file\n",
    "    with open(os.path.join(outdir, \"1.InputSeq_\" + t + \".fasta\"), \"w\") as f:\n",
    "        for i, base in enumerate(basepos):\n",
    "            if isinstance(base[0], int):  # no variants found\n",
    "                f.write(\"%s, %d to %d\\n\" % (headers[i], base[0] + 1, base[1] + 1))\n",
    "                f.write(\"%s\\n\\n\" % sequences[i])\n",
    "            else:  # more than one variants found\n",
    "                for j, subbase in enumerate(base):\n",
    "                    f.write(\n",
    "                        \"%s, %d to %d\\n\" % (headers[i], subbase[0] + 1, subbase[1] + 1)\n",
    "                    )\n",
    "                    f.write(\"%s\\n\\n\" % sequences[i][j])\n",
    "    temp = readfastafile.readfasta(os.path.join(outdir, \"1.InputSeq_\" + t + \".fasta\"))\n",
    "    headers_wpos = temp[1]\n",
    "    sequences = temp[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e1917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7da80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit = \"ENSMUST00000001051\"\n",
    "variants = [hit]\n",
    "\n",
    "# Strip version suffixes for comparison\n",
    "hit_core = hit.split(\".\")[0]\n",
    "if isinstance(variants[0], list):  # multiple variants\n",
    "    variant_cores = [v.split(\".\")[0] for v in variants[0]]\n",
    "elif isinstance(variants[0], str):  # one variant\n",
    "    variant_cores = [variants[0].split(\".\")[0]]\n",
    "\n",
    "hit_core, variant_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b088cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "genepars = (genes, linkers, headers, variants)\n",
    "genes, linkers, headers, variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "variants\n",
    "variant_cores = [variants[0].split(\".\")[0]]\n",
    "variant_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce09656",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.annotation_file:\n",
    "    print(\"bananas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1500375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_core = hit.split(\".\")[0]\n",
    "if len(variants) > 1:\n",
    "    variant_cores = [v.split(\".\")[0] for v in variants[0]]\n",
    "elif len(variants) == 1:\n",
    "    variant_cores = [variants[0].split(\".\")[0]]\n",
    "\n",
    "print(hit_core, variant_cores)\n",
    "if hit_core not in variant_cores:\n",
    "    print(\"Non-specific hit found:\", hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be3911",
   "metadata": {},
   "outputs": [],
   "source": [
    "designinput = (basepos, headers_wpos, sequences, variants_matching_sequence)\n",
    "basepos, headers_wpos, sequences, variants_matching_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ce453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tm, siteChopped = screenseq.thresholdtm(\n",
    "    designinput[1],\n",
    "    designinput[2],\n",
    "    outpars[1],\n",
    "    designpars,  # armlen used, but not important, only total length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615655f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make blast database if it doesn't exist\n",
    "formatrefseq.blastdb(designpars[0])\n",
    "# Run blast in parallel\n",
    "parblast.continueblast(siteChopped, designinput[1], outpars[1], designpars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebedba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "siteCandidates, notMapped = readblast.getcandidates(\n",
    "    siteChopped,\n",
    "    designinput[1],\n",
    "    outpars,\n",
    "    designpars[1],\n",
    "    designinput[3],\n",
    "    config.specificity_by_tm,\n",
    "    designpars[6],  # total length\n",
    ")  # used in readblastout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8820825a",
   "metadata": {},
   "source": [
    "## Building SNAIL probes\n",
    "\n",
    "I need to \n",
    "\n",
    "- Split the binding regions\n",
    "- Source barcodes\n",
    "- Source a sequencing primer\n",
    "- Build random primer-padlock overlaps\n",
    "- Assemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8384e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source all specific targets\n",
    "outdir = Path(\"/nemo/lab/znamenskiyp/scratch\")\n",
    "panel_name = \"spapros_10K_taylored_2021_21_ensembl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad03249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_candidates(outdir, panel_name, probefile=\"3.AllSpecificTargets_*.csv\"):\n",
    "    \"\"\"\n",
    "    Read and concatenate all \"3.AllSpecificTargets_*.csv\" files from subdirectories of the given panel directory.\n",
    "\n",
    "    Args:\n",
    "        outdir (Path): Base output directory containing the panel subdirectory.\n",
    "        panel_name (str): Name of the panel subdirectory.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated dataframe with columns ['target', 'Tm', 'startpos', 'gene'].\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    panel_dir = outdir / panel_name\n",
    "\n",
    "    # Loop over subdirectories\n",
    "    for subdir in panel_dir.iterdir():\n",
    "        if subdir.is_dir():\n",
    "            print(f\"Processing {subdir.name}\")\n",
    "\n",
    "            # Collect \"3.AllSpecificTargets_*.csv\"\n",
    "            target_files = list(subdir.glob(probefile))\n",
    "            if len(target_files) != 1:\n",
    "                print(\n",
    "                    f\"Warning: Expected exactly one target file in {subdir}, found {len(target_files)}. Skipping.\"\n",
    "                )\n",
    "                continue\n",
    "            tf = target_files[0]\n",
    "\n",
    "            # Load and tag with gene name (= subdir name)\n",
    "            with open(tf) as f:\n",
    "                clean_lines = [line for line in f if not line.startswith(\">\")]\n",
    "                clean_lines = [smart_split(line.strip()) for line in clean_lines]\n",
    "\n",
    "            header, *data = clean_lines\n",
    "            if len(data) == 0:\n",
    "                print(f\"Warning: No data found in {tf}, skipping.\")\n",
    "                continue\n",
    "            if len(data[0]) < 2:\n",
    "                print(f\"Warning: No data found in {tf}, skipping.\")\n",
    "                continue\n",
    "            df = pd.DataFrame(data, columns=header)\n",
    "            df[\"gene\"] = subdir.name\n",
    "            if len(df) < 1:\n",
    "                print(f\"Warning: No targets found in {subdir.name}, skipping.\")\n",
    "                continue\n",
    "            all_dfs.append(df)\n",
    "\n",
    "    # Concatenate all into one big dataframe (OUTSIDE the loop)\n",
    "    if all_dfs:\n",
    "        big_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    else:\n",
    "        big_df = pd.DataFrame()  # Return empty DataFrame if no data\n",
    "\n",
    "    big_df = big_df[big_df[\"acronym\"].notna() & (big_df[\"acronym\"] != \"\")]\n",
    "\n",
    "    return big_df\n",
    "\n",
    "\n",
    "def smart_split(line):\n",
    "    # split on commas not inside [ ... ]\n",
    "    return re.split(r\",(?![^[]*\\])\", line)\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "big_df = read_candidates(outdir, panel_name, probefile=\"6.ProbesRandomSubset*.csv\")\n",
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2063a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"AATGACCAACTCCTCCTCCACTTCGACCTCCACCACTACCGGGG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b16136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_gene = big_df.groupby(\"acronym\")[\"target\"].count()\n",
    "\n",
    "# Sort values for clarity\n",
    "per_gene_sorted = per_gene.sort_values(ascending=False)\n",
    "\n",
    "# Make figure tall enough for ~100 bars\n",
    "plt.figure(figsize=(len(per_gene_sorted) * 0.3, 10))\n",
    "\n",
    "per_gene_sorted.plot(kind=\"bar\")\n",
    "\n",
    "plt.xlabel(\"Gene Acronym\")\n",
    "plt.axhline(y=4, color=\"red\")\n",
    "plt.ylabel(\"Target Count\")\n",
    "plt.title(\"Target Counts per Gene Acronym\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef0606",
   "metadata": {},
   "source": [
    "- Design unique overlaps per probe-primer pair\n",
    "- Split the target, build skeleton\n",
    "- Add barcode\n",
    "- Add UMI\n",
    "- Add random specifier\n",
    "- Add sequencing primer\n",
    "- Verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f297d3",
   "metadata": {},
   "source": [
    "So let's start by defining the unique overlaps and building the skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24160e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1, 2, 3, 4\n",
    "\n",
    "probe_ids = [1, 2, 3, 4, 11, 12, 13, 14]\n",
    "genes = big_df[\"acronym\"].unique().tolist()\n",
    "\n",
    "probes = np.array([f\"{gene}_{pid}\" for gene in genes for pid in probe_ids])\n",
    "padlocks = np.array([f\"{gene}_{pid}\" for gene in genes for pid in [1, 2, 3, 4]])\n",
    "gene_array = np.array([gene for gene in genes for pid in probe_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0fd65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(probes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc426f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaps = makesnails.generate_random_dna_sequences(\n",
    "    12, 12, len(genes) * 4, True, genes=padlocks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c16e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure primer and padlock for each pair share the same overlap sequence\n",
    "# Create a list to store all rows (original + duplicates)\n",
    "new_rows = []\n",
    "\n",
    "# Iterate through the dataframe in groups of 4\n",
    "for i in range(0, len(overlaps), 4):\n",
    "    # Get the current group of 4 rows\n",
    "    group = overlaps.iloc[i : i + 4]\n",
    "\n",
    "    # Add the original 4 rows\n",
    "    new_rows.append(group)\n",
    "\n",
    "    # Create duplicates with modified IDs\n",
    "    duplicates = group.copy()\n",
    "    duplicates[\"Overlap Primer Sequence\"] = duplicates[\n",
    "        \"Overlap Primer Sequence\"\n",
    "    ].str.replace(r\"_(\\d+)$\", lambda m: f\"_{int(m.group(1)) + 10}\", regex=True)\n",
    "\n",
    "    # Add the duplicate 4 rows\n",
    "    new_rows.append(duplicates)\n",
    "\n",
    "# Concatenate all rows and reset index\n",
    "overlaps = pd.concat(new_rows, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a47f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367517a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "probes_df = pd.DataFrame(\n",
    "    {\n",
    "        \"probe_id\": probes,\n",
    "        \"gene\": gene_array,\n",
    "        \"overlap_sequence\": overlaps[\"Sequence\"],\n",
    "        \"overlaps_reverse_complement\": overlaps[\"Reverse Complement\"],\n",
    "        \"overlaps_Tms\": overlaps[\"Tms\"],\n",
    "        \"Tm\": np.zeros(len(probes)),\n",
    "        \"startpos\": np.zeros(len(probes), dtype=int),\n",
    "        \"endpos\": np.zeros(len(probes), dtype=int),\n",
    "        \"transcript_region\": np.zeros(len(probes), dtype=int),\n",
    "        \"target\": np.zeros(len(probes)),\n",
    "    }\n",
    ")\n",
    "probes_df.set_index(\"probe_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a0748e",
   "metadata": {},
   "source": [
    "Now, for each gene we can choose four targets and assemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf955c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "seed = 42  # set None for non-deterministic picks\n",
    "selected = []\n",
    "\n",
    "for gene in genes:\n",
    "    probes_slice = probes_df[\"gene\"] == gene\n",
    "    candidates = big_df[big_df[\"acronym\"] == gene]\n",
    "    if candidates.empty:\n",
    "        print(f\"Warning: no targets for {gene}\")\n",
    "        continue\n",
    "    if candidates.shape[0] < 4:\n",
    "        print(f\"Warning: only {candidates.shape[0]} targets for {gene}, fewer than 4\")\n",
    "        continue\n",
    "    n = min(4, len(candidates))\n",
    "    chosen = candidates.sample(n=n, replace=False, random_state=seed)\n",
    "    for pair in [1, 2, 3, 4]:\n",
    "        padlock = f\"{gene}_{pair}\"\n",
    "        primer = f\"{gene}_{pair+10}\"\n",
    "        target = chosen.iloc[pair - 1]\n",
    "\n",
    "        # Assign to padlock\n",
    "        probes_df.loc[padlock, \"target\"] = target[\"target\"]\n",
    "        probes_df.loc[padlock, \"Tm\"] = target[\"Tm\"]\n",
    "        probes_df.loc[padlock, \"startpos\"] = target[\"startpos\"]\n",
    "        probes_df.loc[padlock, \"endpos\"] = target[\"endpos\"]\n",
    "        probes_df.loc[padlock, \"transcript_region\"] = target[\"transcript_region\"]\n",
    "        probes_df.loc[padlock, \"type\"] = \"padlock\"\n",
    "\n",
    "        # Assign to primer (same values)\n",
    "        probes_df.loc[primer, \"target\"] = target[\"target\"]\n",
    "        probes_df.loc[primer, \"Tm\"] = target[\"Tm\"]\n",
    "        probes_df.loc[primer, \"startpos\"] = target[\"startpos\"]\n",
    "        probes_df.loc[primer, \"endpos\"] = target[\"endpos\"]\n",
    "        probes_df.loc[primer, \"transcript_region\"] = target[\"transcript_region\"]\n",
    "        probes_df.loc[primer, \"type\"] = \"primer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7625fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "probes_df[\"sequencing_primer\"] = \"GTCGGACTGTAGAACTCT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3e483a",
   "metadata": {},
   "source": [
    "Sample from the chosen barcode set. In this case, we will use the constrained panel that we optimised by greedy search of the linked loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4957115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5aa296",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_7_path = \"/nemo/lab/znamenskiyp/home/users/colasa/code/multi_padlock_design/constrained_panel/224_subset_prefix7_d3_seed59.txt\"\n",
    "all_10_path = \"/nemo/lab/znamenskiyp/home/users/colasa/code/multi_padlock_design/constrained_panel/1800_d4_10bp_barcodes_improved.txt\"\n",
    "\n",
    "with open(subset_7_path) as f:\n",
    "    lines = f.readlines()\n",
    "    short_barcodes = [line.split(\",\")[1].strip() for line in lines]\n",
    "\n",
    "with open(all_10_path) as f:\n",
    "    lines = f.readlines()\n",
    "    all_barcodes = [line.split(\",\")[1].strip() for line in lines]\n",
    "\n",
    "extra_barcodes = [barcode for barcode in all_barcodes if barcode not in short_barcodes]\n",
    "\n",
    "gene_dict = pd.DataFrame(\n",
    "    {\n",
    "        \"genes\": genes + [np.nan] * (len(short_barcodes + extra_barcodes) - len(genes)),\n",
    "        \"barcodes\": short_barcodes + extra_barcodes,\n",
    "        \"is_7_mer\": [[True] * len(short_barcodes) + [False] * len(extra_barcodes)][0],\n",
    "    }\n",
    ")\n",
    "\n",
    "for item in gene_dict[\"barcodes\"][gene_dict[\"is_7_mer\"] == True]:\n",
    "    assert item in short_barcodes\n",
    "for item in gene_dict[\"barcodes\"][gene_dict[\"is_7_mer\"] == False]:\n",
    "    assert item in extra_barcodes\n",
    "\n",
    "gene_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec99cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "probes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce0b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Generate all 4-letter combinations of ATCG\n",
    "all_combinations = [\"\".join(combo) for combo in itertools.product(\"ATCG\", repeat=4)]\n",
    "\n",
    "# Define length-3 homopolymers to remove\n",
    "homopolymers = [\"AAA\", \"TTT\", \"CCC\", \"GGG\"]\n",
    "\n",
    "# Remove sequences containing any homopolymer\n",
    "filtered_combinations = [\n",
    "    seq for seq in all_combinations if not any(hp in seq for hp in homopolymers)\n",
    "]\n",
    "\n",
    "# Choose random n sequences\n",
    "n = len(genes)  # Change this to however many you want\n",
    "random_selection = random.sample(filtered_combinations, n)\n",
    "\n",
    "extra_redundancy = pd.DataFrame(\n",
    "    {\"genes\": genes, \"barcode\": random_selection}\n",
    ").set_index(\"genes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea97d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_probes = []\n",
    "UMI = [\"A\", \"T\", \"C\", \"G\"]\n",
    "for gene in genes:\n",
    "    subset = probes_df[probes_df[\"gene\"] == gene].copy\n",
    "    subset[\"barcode\"] = gene_dict[\"barcodes\"][gene_dict[\"gene\"] == gene]\n",
    "    for pair in [1, 2, 3, 4]:\n",
    "        padlock = f\"{gene}_{pair}\"\n",
    "        primer = f\"{gene}_{pair+10}\"\n",
    "\n",
    "        subset.loc[padlock, \"extra_redundancy\"] = extra_redundancy[gene]\n",
    "        subset.loc[padlock, \"target_seq\"] = makesnails.revcomp(\n",
    "            subset.loc[padlock, target][0:22]\n",
    "        )\n",
    "        subset.loc[padlock, \"UMI\"] = UMI[pair - 1]\n",
    "        subset.loc[padlock, \"overlap_5_seq\"] = subset.loc[padlock,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf9b38",
   "metadata": {},
   "source": [
    "For each site, add the reverse compliment and break it at site 22. Add primer, spacer and padlock. Calculate the delta Tm between primer and padlock arms, add it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c91922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ee4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df[\"reverse_comp\"] = big_df[\"target\"].apply(makesnails.revcomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff277431",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df[\"padlock\"] = [big_df[\"reverse_comp\"][i][0:22] for i in range(len(big_df))]\n",
    "big_df[\"spacer\"] = [big_df[\"reverse_comp\"][i][22:24] for i in range(len(big_df))]\n",
    "big_df[\"primer\"] = [big_df[\"reverse_comp\"][i][24:] for i in range(len(big_df))]\n",
    "\n",
    "big_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df[\"padlock_tm\"] = [\n",
    "    readblast.calc_tm_NN(big_df[\"padlock\"][i]) for i in range(len(big_df))\n",
    "]\n",
    "big_df[\"primer_tm\"] = [\n",
    "    readblast.calc_tm_NN(big_df[\"primer\"][i]) for i in range(len(big_df))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e2d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generate_random_dna_sequences(\n",
    "    min_barcode, length, num_sequences, for_overlap, genes=None\n",
    "):\n",
    "    bases = [\"A\", \"T\", \"C\", \"G\"]\n",
    "    sequences = []\n",
    "    reverse_complements = []\n",
    "    Tms = []\n",
    "\n",
    "    while len(sequences) < num_sequences:\n",
    "        sequence = \"\".join(random.choice(bases) for _ in range(length))\n",
    "\n",
    "        # Check condition for overlap oligos\n",
    "        if (\n",
    "            for_overlap\n",
    "            and sequence[3:6]\n",
    "            in [\"TAA\", \"TCA\", \"TGA\", \"TTA\", \"ATT\", \"AGT\", \"ACT\", \"AAT\"]\n",
    "        ):  # ligation wont happen when reverse complement is TXA In 6:9 position for overlap\n",
    "            continue\n",
    "\n",
    "        # No homopolymers\n",
    "        if (\n",
    "            (\"AAA\" in sequence)\n",
    "            or (\"TTT\" in sequence)\n",
    "            or (\"CCC\" in sequence)\n",
    "            or (\"GGG\" in sequence)\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        # check Tm?\n",
    "        Tm = readblast.calc_tm_NN(sequence)\n",
    "        if Tm < 25 or Tm > 35:\n",
    "            continue\n",
    "\n",
    "        if all(\n",
    "            count_diff(seq[:min_barcode], sequence[:min_barcode]) >= 2\n",
    "            for seq in sequences\n",
    "        ) and all(count_diff(seq, sequence) >= 4 for seq in sequences):\n",
    "            sequences.append(sequence)\n",
    "            complement = {\"A\": \"T\", \"T\": \"A\", \"C\": \"G\", \"G\": \"C\"}\n",
    "            reverse_seq = sequence[::-1]\n",
    "            reverse_complement_seq = \"\".join(complement[base] for base in reverse_seq)\n",
    "            reverse_complements.append(reverse_complement_seq)\n",
    "            Tms.append(Tm)\n",
    "\n",
    "    # Build dataframe\n",
    "    df = pd.DataFrame(\n",
    "        {\"Sequence\": sequences, \"Reverse Complement\": reverse_complements, \"Tms\": Tms}\n",
    "    ).drop_duplicates()\n",
    "\n",
    "    # Handle gene names\n",
    "    if genes is None:\n",
    "        genes = [\"no_gene\"] * len(df)\n",
    "    else:\n",
    "        # pad or truncate to match df length\n",
    "        if len(genes) < len(df):\n",
    "            genes = genes + [\"no_gene\"] * (len(df) - len(genes))\n",
    "        elif len(genes) > len(df):\n",
    "            genes = genes[: len(df)]\n",
    "\n",
    "    df[\"Overlap Primer Sequence\"] = genes\n",
    "    df[\"ID\"] = range(len(df))  # unique IDs\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def count_diff(seq1, seq2):\n",
    "    return sum(base1 != base2 for base1, base2 in zip(seq1, seq2))\n",
    "\n",
    "\n",
    "# Example usage\n",
    "genes = big_df\n",
    "\n",
    "overlap_prims = generate_random_dna_sequences(12, 12, 14, for_overlap=True, genes=genes)\n",
    "print(overlap_prims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989e1296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08632e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = list(big_df.index)  # make a unique overlap for all sequences\n",
    "overlap_regions = makesnails.generate_random_dna_sequences(\n",
    "    12, 12, len(genes), True, genes=genes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9729e21",
   "metadata": {},
   "source": [
    "## Barcodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b8b649",
   "metadata": {},
   "source": [
    "Check the Hamming distance matrix between all the candidates I have, have a look at how many fit in the barcode space at 7bp, decide how much room for a larger panel I want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6f080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "barcodes = pd.read_csv(\n",
    "    \"/nemo/lab/znamenskiyp/home/users/colasa/code/multi_padlock_design/old_7bp_barcodes.csv\",\n",
    "    names=[\"ID\", \"barcode\"],\n",
    ")\n",
    "barcodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244034d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"/nemo/lab/znamenskiyp/home/users/colasa/code/multi_padlock_design/nested_barcodes_parallel/2097_d4_10bp_barcodes_seed105.txt\"\n",
    ") as f:\n",
    "    lines = [line.strip().split(\",\") for line in f if line.strip()]\n",
    "\n",
    "ids = [x[0] for x in lines]\n",
    "all_code = [x[1] for x in lines]\n",
    "\n",
    "barcodes = pd.DataFrame({\"ID\": ids, \"barcode\": all_code})\n",
    "\n",
    "print(max([len(a) for a in barcodes[\"barcode\"]]))\n",
    "\n",
    "print(min([len(a) for a in barcodes[\"barcode\"]]))\n",
    "\n",
    "# eliminate the two final ones\n",
    "# seven_mers = [barcode[:-3] for barcode in barcodes['barcode']]\n",
    "# barcodes['barcode']=seven_mers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc9169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert barcodes to a 2D numpy array of characters\n",
    "barcodes_array = np.array([list(seq) for seq in barcodes[\"barcode\"]])\n",
    "\n",
    "# Broadcast and compare all pairs at once\n",
    "# shape: (n, n, L) where n = number of barcodes, L = length of each\n",
    "diffs = barcodes_array[:, None, :] != barcodes_array[None, :, :]\n",
    "\n",
    "# Sum mismatches along sequence length (axis=2)\n",
    "dist_matrix = diffs.sum(axis=2)\n",
    "\n",
    "# Put result in a dataframe\n",
    "dist_df = pd.DataFrame(dist_matrix, index=barcodes[\"ID\"], columns=barcodes[\"ID\"])\n",
    "print(dist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a4db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dist_df.values, cmap=\"viridis\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3409e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dist_df.values.flatten(), bins=range(0, 9), align=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a63ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.power(4, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcef13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.power(4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d524e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = \"ACGT\"\n",
    "n = 7\n",
    "\n",
    "# as a list of strings\n",
    "all_barcodes = [\"\".join(p) for p in product(alphabet, repeat=n)]\n",
    "print(len(all_barcodes))  # 16384\n",
    "print(all_barcodes[:5])  # first few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2faf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_barcode_set(candidates, d):\n",
    "    # candidates: list/array of strings, equal length\n",
    "    chosen = []\n",
    "    arr = np.array([list(s) for s in candidates])  # shape (K, n)\n",
    "\n",
    "    for idx, s in enumerate(arr):\n",
    "        if not chosen:\n",
    "            chosen.append(idx)  # seed the list\n",
    "            continue\n",
    "        # compare s to all chosen in a vectorized way\n",
    "        prev = arr[chosen]  # (m, n)\n",
    "        # number of mismatches vs each chosen codeword\n",
    "        dist = (prev != s).sum(axis=1)\n",
    "        if np.all(dist >= d):\n",
    "            chosen.append(idx)\n",
    "    return [candidates[i] for i in chosen]\n",
    "\n",
    "\n",
    "candidates = greedy_barcode_set(all_barcodes, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c94810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(a, b):\n",
    "    # assumes equal length strings\n",
    "    return sum(ch1 != ch2 for ch1, ch2 in zip(a, b))\n",
    "\n",
    "\n",
    "def greedy_once(candidates, d):\n",
    "    chosen = []\n",
    "    for cand in candidates:\n",
    "        if all(hamming_distance(cand, c) >= d for c in chosen):\n",
    "            chosen.append(cand)\n",
    "    return chosen\n",
    "\n",
    "\n",
    "def multi_restart_greedy(n=7, alphabet=\"ACGT\", d=3, restarts=20, seed=None):\n",
    "    # generate all candidates\n",
    "    candidates = [\"\".join(p) for p in product(alphabet, repeat=n)]\n",
    "    best = []\n",
    "    rng = random.Random(seed)\n",
    "    for _ in tqdm(range(restarts)):\n",
    "        rng.shuffle(candidates)\n",
    "        chosen = greedy_once(candidates, d)\n",
    "        if len(chosen) > len(best):\n",
    "            best = chosen\n",
    "    return best\n",
    "\n",
    "\n",
    "# Example: try to build a code with n=7, d=3\n",
    "barcodes = multi_restart_greedy(n=10, d=4, restarts=50, seed=55)\n",
    "print(\"Size:\", len(barcodes))\n",
    "print(\"First few:\", barcodes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "barcodes_len_3 = barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(a, b):\n",
    "    return sum(ch1 != ch2 for ch1, ch2 in zip(a, b))\n",
    "\n",
    "\n",
    "def local_improvement(chosen, candidates, d, max_passes=3):\n",
    "    chosen = chosen[:]  # copy\n",
    "    for _ in range(max_passes):\n",
    "        improved = False\n",
    "        for x in candidates:\n",
    "            if x in chosen:\n",
    "                continue\n",
    "            conflicts = [c for c in chosen if hamming_distance(x, c) < d]\n",
    "            # Add if it fits cleanly\n",
    "            if not conflicts:\n",
    "                chosen.append(x)\n",
    "                improved = True\n",
    "            # Simple swap if it blocks only one\n",
    "            elif len(conflicts) == 1:\n",
    "                c = conflicts[0]\n",
    "                chosen.remove(c)\n",
    "                if all(hamming_distance(x, c2) >= d for c2 in chosen):\n",
    "                    chosen.append(x)\n",
    "                    improved = True\n",
    "                else:\n",
    "                    chosen.append(c)  # revert\n",
    "        if not improved:\n",
    "            break\n",
    "    return chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f52db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all barcodes of length 7 over ACGT\n",
    "candidates = [\"\".join(p) for p in product(\"ACGT\", repeat=7)]\n",
    "barcodes = pd.read_csv(\n",
    "    \"/nemo/lab/znamenskiyp/home/users/colasa/code/multi_padlock_design/old_7bp_barcodes.csv\",\n",
    "    names=[\"ID\", \"barcode\"],\n",
    ")\n",
    "greedy_code = barcodes[\"barcode\"].astype(str).tolist()\n",
    "old_size = len(greedy_code)\n",
    "print(\"Old size:\", old_size)\n",
    "\n",
    "improved_code = local_improvement(greedy_code, candidates, d=3, max_passes=5)\n",
    "print(\"Improved size:\", len(improved_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4151168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"369_d3_7bp_barcodes.txt\", \"w\") as f:\n",
    "    for i, code in enumerate(improved_code):\n",
    "        f.write(f\"{i},{code}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3041589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base2int = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3}\n",
    "\n",
    "\n",
    "def encode_barcode(bc):\n",
    "    return np.fromiter((base2int[b] for b in bc), dtype=np.uint8)\n",
    "\n",
    "\n",
    "def encode_list(barcodes):\n",
    "    return np.vstack([encode_barcode(bc) for bc in barcodes])\n",
    "\n",
    "\n",
    "def hamming_vectorized(X, y):\n",
    "    return (X != y).sum(axis=1)\n",
    "\n",
    "\n",
    "def local_improvement_fast(chosen, candidates, d, max_passes=3):\n",
    "    # Encode chosen and candidates\n",
    "    chosen_enc = encode_list(chosen)\n",
    "    cand_enc = encode_list(candidates)\n",
    "    chosen_set = set(chosen)  # quick membership check\n",
    "\n",
    "    for _ in range(max_passes):\n",
    "        improved = False\n",
    "        new_chosen = []\n",
    "        for x_str, x_enc in zip(candidates, cand_enc):\n",
    "            if x_str in chosen_set:\n",
    "                continue\n",
    "            # vectorized distance check\n",
    "            dists = (chosen_enc != x_enc).sum(axis=1)\n",
    "            conflicts = np.where(dists < d)[0]\n",
    "\n",
    "            if len(conflicts) == 0:\n",
    "                chosen.append(x_str)\n",
    "                chosen_enc = np.vstack([chosen_enc, x_enc])\n",
    "                chosen_set.add(x_str)\n",
    "                improved = True\n",
    "            elif len(conflicts) == 1:\n",
    "                # attempt swap\n",
    "                idx = conflicts[0]\n",
    "                tmp_enc = np.delete(chosen_enc, idx, axis=0)\n",
    "                if ((tmp_enc != x_enc).sum(axis=1) >= d).all():\n",
    "                    # swap is valid\n",
    "                    removed = chosen[idx]\n",
    "                    chosen.pop(idx)\n",
    "                    chosen_enc = tmp_enc\n",
    "                    chosen.append(x_str)\n",
    "                    chosen_enc = np.vstack([chosen_enc, x_enc])\n",
    "                    chosen_set.discard(removed)\n",
    "                    chosen_set.add(x_str)\n",
    "                    improved = True\n",
    "        if not improved:\n",
    "            break\n",
    "    return chosen\n",
    "\n",
    "\n",
    "# generate all 7-mers\n",
    "candidates = [\"\".join(p) for p in product(\"ACGT\", repeat=10)]\n",
    "\n",
    "# your initial set\n",
    "greedy_code = greedy_code  # from previous greedy step\n",
    "\n",
    "print(\"Old size:\", len(greedy_code))\n",
    "improved_code = local_improvement_fast(greedy_code, candidates, d=4, max_passes=35)\n",
    "print(\"Improved size:\", len(improved_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cbb8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"/nemo/lab/znamenskiyp/home/users/colasa/code/multi_padlock_design/barcodes_n10_d4.txt\"\n",
    ") as f:\n",
    "    greedy_code = [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"/nemo/lab/znamenskiyp/home/users/colasa/code/multi_padlock_design/2097_d4_10bp_barcodes.txt\"\n",
    ") as f:\n",
    "    all_code = [line.strip().split(\",\")[1] for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f757a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering\n",
    "# Ensure there are no runs of more than `max_homopolymer_length` consecutive bases\n",
    "# and at least three different bases\n",
    "\n",
    "# look at all 10_mers\n",
    "candidates = np.array([\"\".join(p) for p in product(\"ACGT\", repeat=10)])\n",
    "print(f\"Length before constraints {len(candidates)}\")\n",
    "\n",
    "\n",
    "# homopolymer constraint\n",
    "def noHomo(barcode):\n",
    "    if (\n",
    "        (\"AAA\" in barcode)\n",
    "        | (\"TTT\" in barcode)\n",
    "        | (\"CCC\" in barcode)\n",
    "        | (\"GGG\" in barcode)\n",
    "    ):\n",
    "        noHomo = False\n",
    "    else:\n",
    "        noHomo = True\n",
    "    return noHomo\n",
    "\n",
    "\n",
    "noHomo_mask = [noHomo(barcode) for barcode in candidates]\n",
    "candidates = candidates[noHomo_mask]\n",
    "\n",
    "print(f\"length after homopolymers: {len(candidates)}\")\n",
    "\n",
    "diversity_mask = [(len(set(barcode)) >= 3) for barcode in candidates]\n",
    "candidates = candidates[diversity_mask]\n",
    "\n",
    "print(f\"length after diversity: {len(candidates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1810aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"AAATCT\"\n",
    "if (\"AAA\" in a) | (\"AAA\" in a):\n",
    "    print(\"bananas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8596159c",
   "metadata": {},
   "source": [
    "## Debug finding regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\n",
    "    \">ENSMUST00000025404.10 cdna chromosome:GRCm39:18:67476674:67500855:1 gene:ENSMUSG00000024526.10 gene_biotype:protein_coding transcript_biotype:protein_coding gene_symbol:Cidea description:cell death-inducing DNA fragmentation factor, alpha subunit-like effector A [Source:MGI Symbol;Acc:MGI:1270845]\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8127d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for header in headers:\n",
    "    first = header.split()[0]  # take the first token\n",
    "    print(first)\n",
    "    header = first.lstrip(\">\")  # only strip '>' if it exists\n",
    "    print(header)\n",
    "    transcript = header\n",
    "    print(transcript)\n",
    "    cds_file = config.cds_file\n",
    "    cdna_file = config.cdna_file\n",
    "    df = finalizeprobes.find_start_end_sites(cds_file, cdna_file, transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cd0bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\n",
    "    \"ENSMUST00000117695.8 cdna chromosome:GRCm39:19:40917371:40982591:-1 gene:ENSMUSG00000061132.14 gene_biotype:protein_coding transcript_biotype:protein_coding gene_symbol:Blnk description:B cell linker [Source:MGI Symbol;Acc:MGI:96878]\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source all specific targets\n",
    "outdir = Path(\"/nemo/lab/znamenskiyp/scratch\")\n",
    "panel_name = \"spapros_10K_taylored_2021_16_ensembl\"\n",
    "probes_df = makesnails.read_candidates(\n",
    "    outdir, panel_name, probefile=\"5.ProbesDBMappable_*.csv\"\n",
    ")\n",
    "probes_df = probes_df[probes_df[\"acronym\"] == \"Cidea\"]\n",
    "probes = list(probes_df[\"target\"])\n",
    "probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = \"/nemo/lab/znamenskiyp/scratch/spapros_10K_taylored_2021_16_ensembl/Cidea/TempFolder20251027141110\"\n",
    "import glob\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "\n",
    "import config\n",
    "from lib import finalizeprobes, makesnails\n",
    "\n",
    "headers = [\n",
    "    \">ENSMUST00000025404.10 cdna chromosome:GRCm39:18:67476674:67500855:1 gene:ENSMUSG00000024526.10 gene_biotype:protein_coding transcript_biotype:protein_coding gene_symbol:Cidea description:cell death-inducing DNA fragmentation factor, alpha subunit-like effector A [Source:MGI Symbol;Acc:MGI:1270845]\"\n",
    "]\n",
    "import os\n",
    "\n",
    "regions = []\n",
    "\n",
    "\n",
    "def align_to_reference_variant(target_sequence, reference_variant_seq):\n",
    "    # target_sequence = short probe\n",
    "    # reference_variant_seq = long cDNA\n",
    "    matcher = SequenceMatcher(None, reference_variant_seq, target_sequence)\n",
    "    match = matcher.find_longest_match(\n",
    "        0, len(reference_variant_seq), 0, len(target_sequence)\n",
    "    )\n",
    "    return match.a, match.size, reference_variant_seq[match.a : match.a + match.size]\n",
    "\n",
    "\n",
    "def get_gene_symbol(header):\n",
    "    # Split by spaces\n",
    "    parts = header.split()\n",
    "\n",
    "    # Find the token that starts with \"gene_symbol:\"\n",
    "    gene_symbol_field = next((p for p in parts if p.startswith(\"gene:\")), None)\n",
    "\n",
    "    # Extract the value after the colon\n",
    "    if gene_symbol_field:\n",
    "        gene_symbol = gene_symbol_field.split(\":\", 1)[1]\n",
    "    else:\n",
    "        gene_symbol = None\n",
    "\n",
    "    return gene_symbol\n",
    "\n",
    "\n",
    "def find_latest_variants_fasta(tempdir):\n",
    "    # Look for files like variants_round3.fasta in the given directory\n",
    "    pattern = os.path.join(tempdir, \"*variants_round*.fasta\")\n",
    "    round_files = glob.glob(pattern)\n",
    "\n",
    "    if round_files:\n",
    "        print(\"found some variant files\")\n",
    "\n",
    "        # Extract round numbers using regex and pick the highest one\n",
    "        def round_num(fpath):\n",
    "            fname = os.path.basename(fpath)\n",
    "            m = re.search(r\"variants_round(\\d+)\\.fasta$\", fname)\n",
    "            return int(m.group(1)) if m else -1\n",
    "\n",
    "        latest = max(round_files, key=round_num)\n",
    "        print(f\"picking latest: {latest}\")\n",
    "\n",
    "        with open(latest, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            headers = [line.strip() for line in lines if line.startswith(\">\")]\n",
    "            print(headers)\n",
    "            variants = [h.lstrip(\">\").split()[0] for h in headers]\n",
    "\n",
    "        return variants\n",
    "\n",
    "    # Fallback: look for variants.fasta\n",
    "    pattern = os.path.join(tempdir, \"*variants.fasta\")\n",
    "    round_files = glob.glob(pattern)\n",
    "    try:\n",
    "        plain_file = round_files[0]\n",
    "    except IndexError:\n",
    "        print(\"No variant files found\")\n",
    "        return None\n",
    "    if os.path.exists(plain_file):\n",
    "        with open(plain_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            headers = [line.strip() for line in lines if line.startswith(\">\")]\n",
    "            print(headers)\n",
    "            variants = [h.lstrip(\">\").split()[0] for h in headers]\n",
    "        return variants\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_cds_entries(cds_headers_file, gene_symbol):\n",
    "    entries = []\n",
    "    with open(cds_headers_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        cds_headers = [line.strip() for line in lines]\n",
    "    for header in cds_headers:\n",
    "        if f\"gene:{gene_symbol}\" in header:\n",
    "            entries.append(header)\n",
    "    return entries\n",
    "\n",
    "\n",
    "def get_cdna(transcript_id):\n",
    "    \"\"\"\n",
    "    Return the cDNA sequence (as a string) for a given transcript ID\n",
    "    from a multi-entry FASTA file.\n",
    "    \"\"\"\n",
    "    for record in SeqIO.parse(config.cdna_file, \"fasta\"):\n",
    "        if record.id == transcript_id:\n",
    "            return str(record.seq)\n",
    "    raise ValueError(f\"Transcript ID {transcript_id} not found in {config.cdna_file}\")\n",
    "\n",
    "\n",
    "for header in headers:\n",
    "    # get the ensembl gene symbol\n",
    "    print(header)\n",
    "    gene_symbol = get_gene_symbol(header)\n",
    "    # find entries in the cds database for this gene symbol\n",
    "    cds_headers = str(config.cds_file) + \"_headers.txt\"\n",
    "    print(cds_headers)\n",
    "    cds_entries = find_cds_entries(cds_headers, gene_symbol)\n",
    "    print(cds_entries)\n",
    "    # Iterate, check if it's in the variant list, if it's not, bad luck, complain.\n",
    "    # define variant list\n",
    "    # obtain tempdir from outpars TODO\n",
    "    variants = find_latest_variants_fasta(tempdir)\n",
    "    print(variants)\n",
    "    for cds_variants in cds_entries:\n",
    "        variant_id = cds_variants.split()[0].lstrip(\">\")\n",
    "        if variant_id in variants:\n",
    "            print(f\"Found valid (some annotated cds) variant: {variant_id}\")\n",
    "            reference_variant = variant_id\n",
    "            break\n",
    "        else:\n",
    "            print(f\"No valid (some annotated cds) variant for: {variant_id}\")\n",
    "    # map the target to the reference variant sequence\n",
    "    reference_variant_sequence = get_cdna(reference_variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c1e913",
   "metadata": {},
   "outputs": [],
   "source": [
    "makesnails.revcomp(\"ATGGACAAGCTGAATAAGATAACTGTCCCTGCCAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820310d3",
   "metadata": {},
   "source": [
    "Trying target `GAGCCTTACCATGCTGCTCCCCAGGAAGTCCAGGAGCTGCTGAC`. It is not on the CDS, and it actually is at the very begginning of the transcript, at position 10 of `ENSMUST00000117695.8`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa1183",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\n",
    "    \"\"\"CTTGAGGCCAGAGCCTTACCATGCTGCTCCCCAGGAAGTCCAGGAGCTGCTGACACCCCC\n",
    "    CTGGACAGCGACACATCCTCTCTCATGGACAAGCTGAATAAGATAACTGTAAGAAACCCT\n",
    "    GCCAGCCAGAAGCTGAGACAGCTTCAAAAGATGGTCCATGATATTAAGAACAATGAAGGT\n",
    "    GGAATAATGGACAAGATAAAAAAGCTAAAAGTCAAAGGCCCTCCAAGTGTTCCTCGAAGG\n",
    "    GACTACGCATTAGACAGCCCTGCAGATGAAGAGGAGCAGTGGTCAGATGACTTCGACAGT\n",
    "    GACTATGAAAATCCAGATGAACATTCGGACTCCGAGATGTATGTGATGCCTGCCGAGGAG\n",
    "    ACGGGCGACGATTCCTATGAACCGCCTCCCGCTGAGCAGCAGACACGGGTGGTCCATCCA\n",
    "    GCCCTGCCCTTCACGAGGGGCGAGTATGTAGATAATCGATCCAGCCAGCGGCACTCTCCG\n",
    "    CCCTTCAGCAAGACACTTCCCAGTAAGCCCAGCTGGCCTTCAGCGAAAGCGAGGCTGGCC\n",
    "    TCCACTCTGCCAGCCCCCAACTCTCTACAGAAGCCTCAAGTCCCCCCCAAGCCCAAAGAC\n",
    "    CTCCTTGAGGATGAGGCTGATTATGTGGTCCCTGTGGAAGATAACGATGAAAACTATATC\n",
    "    CATCCCAGAGAAAGCAGCCCGCCGCCTGCTGAGAAGGCTCCCATGGTGAATAGATCAACC\n",
    "    AAGCCAAACAGTTCCTCAAAGCACATGTCGCCTCCAGGGACTGTCGCAGGTCGAAACAGT\n",
    "    GGGGTCTGGGACTCCAAGTCATCTTTGCCTGCCGCACCATCCCCACTACCACGGGCTGGG\n",
    "    AAGAAGCCAGCTACACCACTTAAGACTACTCCCGTTCCTCCCCTACCGAATGCATCAAAT\n",
    "    GTTTGTGAAGAAAAGCCTGTTCCTGCTGAGCGCCACCGAGGGTCTAGTCACAGACAAGAC\n",
    "    ACTGTACAGTCACCAGTGTTTCCTCCCACCCAGAAACCTGTCCATCAAAAGCCTGTACCC\n",
    "    TTGCCAAAAGCGGGGAGCCCAGCTGCAGATGGACCGTTCCACAGCTTCCCATTTAATTCG\n",
    "    ACGTTTGCAGACCAGGAGGCTGAACTGCTCGGTAAGCCCTGGTATGCTGGCGCCTGTGAC\n",
    "    CGCAAGTCTGCTGAAGAGGCCTTGCACAGATCCAACAAGGATGGATCGTTTCTTATTCGG\n",
    "    AAGAGCTCTGGCCATGATTCCAAGCAGCCGTACACCCTAGTTGCGTTCTTTAACAAGCGA\n",
    "    GTGTATAATATTCCTGTACGGTTTATTGAAGCAACCAAACAGTATGCTTTGGGAAAGAAG\n",
    "    AAAAATGGTGAAGAGTACTTCGGAAGTGTTGTGGAAATCGTCAACAGTCACCAGCACAAC\n",
    "    CCCCTGGTTCTTATTGACAGTCAGAATAACACGAAAGATTCCACGAGACTGAAATATGCT\n",
    "    GTGAAGGTTTCATAACGATACCACGGTTCCAGACATGTCCTCTGTTTCTTCTTTTGAGAA\n",
    "    AACATCATATTCTGGCTATGACTCCTCAGCAGTAAGAGAGAAAAGATGAATGAAGCCACT\n",
    "    GAGGCTTCGTGAATGAATGAATCTACTCCTTCCTAGGGCGTTCACACGAGCTTTTCTATC\n",
    "    ACCTGACCTGACGAAGTCATAGCTGGGGAGGTTCGGTTACTATGATACTAATATTGTCCA\n",
    "    AATAAATGTATTTAAAAGCAA\"\"\".replace(\"\\n\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d55a7",
   "metadata": {},
   "source": [
    "## Filtering variants by `GENCODE Basic` subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049267bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(line, feature):\n",
    "    match = re.search(rf'{feature} \"([^\"]+)\"', line)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "gene_ids = []\n",
    "transcript_ids = []\n",
    "transcript_support_levels = []\n",
    "feature_type = []\n",
    "\n",
    "with open(config.gencode_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\") or line.split(\"\\t\")[2] != \"transcript\":\n",
    "            continue  # skip header lines and gene lines\n",
    "        line = line.strip()\n",
    "        gene_id = extract_features(line, \"gene_id\")\n",
    "        transcript_id = extract_features(line, \"transcript_id\")\n",
    "        transcript_support_level = extract_features(line, \"transcript_support_level\")\n",
    "        if gene_id or transcript_id:\n",
    "            gene_ids.append(gene_id)\n",
    "            transcript_ids.append(transcript_id)\n",
    "            transcript_support_levels.append(transcript_support_level)\n",
    "\n",
    "gencode = pd.DataFrame(\n",
    "    {\n",
    "        \"gene_id\": gene_ids,\n",
    "        \"transcript_id\": transcript_ids,\n",
    "        \"transcript_support_level\": transcript_support_levels,\n",
    "    }\n",
    ")\n",
    "print(gencode.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c1e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique transcripts per gene\n",
    "counts = (\n",
    "    gencode.groupby(\"gene_id\")[\"transcript_id\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"n_transcripts\")\n",
    ")\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(counts[\"n_transcripts\"], bins=50, log=True, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.xlabel(\"Number of unique transcript IDs per gene\")\n",
    "plt.ylabel(\"Number of genes\")\n",
    "plt.title(\"Distribution of GENCODE Basic transcript counts per gene\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b89741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safely convert to numeric; non-numeric become NaN\n",
    "tsl = (\n",
    "    pd.to_numeric(gencode[\"transcript_support_level\"], errors=\"coerce\")\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.hist(tsl, bins=range(0, 6), align=\"left\", color=\"lightgreen\", edgecolor=\"black\")\n",
    "plt.xticks(range(0, 6))\n",
    "plt.xlabel(\"Transcript Support Level\")\n",
    "plt.ylabel(\"Number of Transcripts\")\n",
    "plt.title(\"Distribution of Transcript Support Levels (NA=0)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b698e386",
   "metadata": {},
   "source": [
    "\t{1,2,3,4,5,NA} [0,1]\n",
    "transcripts are scored according to how well mRNA and EST alignments match over its full length:  \n",
    "1 (all splice junctions of the transcript are supported by at least one non-suspect mRNA),  \n",
    "2 (the best supporting mRNA is flagged as suspect or the support is from multiple ESTs),  \n",
    "3 (the only support is from a single EST),   \n",
    "4 (the best supporting EST is flagged as suspect),   \n",
    "5 (no single transcript supports the model structure),   \n",
    "NA (the transcript was not analyzed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e5d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gffpandas.gffpandas as gffpd\n",
    "\n",
    "# read a gencode database, which is a gff file (almost a gb, but only once)\n",
    "gencode = gffpd.read_gff3(config.gencode_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5be17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gencode_attributes = gencode.attributes_to_columns()\n",
    "gencode_attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5299ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = [\"Adamts2\"]\n",
    "species = config.species\n",
    "hits, hitmask = retrieveseq.querygenes(genes, species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c80e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = [\"Adamts2\"]\n",
    "species = config.species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547910ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load formatted RefSeq header and sequence files\"\"\"\n",
    "global Acronyms\n",
    "global Headers\n",
    "global Seq\n",
    "global Gencode\n",
    "\n",
    "try:\n",
    "    fastadir = os.path.join(config.BASE_DIR, config.reference_transcriptome)\n",
    "    if getattr(config, \"reference_transcriptome\", \"\").lower() == \"refseq\":\n",
    "        # Expect config.fasta_pre_suffix_refseq = (prefix, suffix), config.fasta_filenum_refseq = int\n",
    "        pre_suf = getattr(config, \"fasta_pre_suffix_refseq\", None)\n",
    "        filenum = getattr(config, \"fasta_filenum_refseq\", None)\n",
    "        acr_path = os.path.join(fastadir, species + \".acronymheaders.txt\")\n",
    "        if not os.path.isfile(acr_path):\n",
    "            print(\"Processing fasta database files..\")\n",
    "            if (\n",
    "                isinstance(pre_suf, (list, tuple))\n",
    "                and len(pre_suf) == 2\n",
    "                and isinstance(filenum, int)\n",
    "            ):\n",
    "                fastadb(\n",
    "                    fastadir,\n",
    "                    (pre_suf[0], pre_suf[1], filenum),\n",
    "                    species,\n",
    "                    source=\"refseq\",\n",
    "                    keep_nm_nr_only=True,\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Invalid RefSeq config: expected fasta_pre_suffix_refseq=(prefix, suffix) and fasta_filenum_refseq=int\"\n",
    "                )\n",
    "\n",
    "    elif getattr(config, \"reference_transcriptome\", \"\").lower() == \"ensembl\":\n",
    "        files = [config.cdna_file]\n",
    "        if hasattr(config, \"extra_files\") and config.extra_files:\n",
    "            files += config.extra_files\n",
    "        acr_path = os.path.join(fastadir, species + \".acronymheaders.txt\")\n",
    "        if not os.path.isfile(acr_path):\n",
    "            print(\"Processing fasta database files..\")\n",
    "            fastadb(\n",
    "                fastadir,\n",
    "                files,\n",
    "                species,\n",
    "                source=\"ensembl\",\n",
    "                keep_nm_nr_only=False,\n",
    "            )\n",
    "\n",
    "    # load database files\n",
    "    with open(os.path.join(fastadir, species + \".acronymheaders.txt\"), \"r\") as f:\n",
    "        Acronyms = [line.rstrip(\"\\n\") for line in f]\n",
    "    with open(os.path.join(fastadir, species + \".selectedheaders.txt\"), \"r\") as f:\n",
    "        Headers = [line.rstrip(\"\\n\") for line in f]\n",
    "    with open(os.path.join(fastadir, species + \".selectedseqs.txt\"), \"r\") as f:\n",
    "        Seq = [line.rstrip(\"\\n\") for line in f]\n",
    "    Gencode = retrieveseq.load_gencode()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Cannot load fasta database due to mismatch in species.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d219f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "Headers[hits[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db354733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensembl_id(header):\n",
    "    match = re.search(r\"gene:([^ ]+)\", header)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "get_ensembl_id(Headers[hits[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c140ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = []\n",
    "gene = genes[0]\n",
    "ensembl_ID = [get_ensembl_id(header) for header in Headers if gene in header][0]\n",
    "print(f\"Querying for {ensembl_ID}\")\n",
    "# filter by Gencode basic annotation\n",
    "allowed_variants = Gencode[Gencode[\"gene_id\"] == ensembl_ID][\"transcript_id\"].tolist()\n",
    "supporting_evidence = Gencode[Gencode[\"gene_id\"] == ensembl_ID][\n",
    "    \"transcript_support_level\"\n",
    "].tolist()\n",
    "print(\n",
    "    f\"allowed variants: {allowed_variants}, supporting evidence: {supporting_evidence} (1 is best)\"\n",
    ")\n",
    "allowed_hit = [\n",
    "    i for i, line in enumerate(Headers) if any(tid in line for tid in allowed_variants)\n",
    "]\n",
    "# now form the list of all variants and provide a mask\n",
    "hit = [c for c, header in enumerate(Acronyms) if header == gene]\n",
    "# filter hits by allowed variants\n",
    "hit_masked = [(Headers[h].split()[0][1:] in allowed_variants) for h in hit]\n",
    "# filter hits by allowed\n",
    "hits.append(hit)\n",
    "\n",
    "hits = (hits, hit_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f94cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(hits[0])[hit_masked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da485f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hit in hits[0]:\n",
    "    print(Headers[hit].split()[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import config\n",
    "from lib import retrieveseq\n",
    "from lib.formatrefseq import fastadb\n",
    "\n",
    "genes = [\"Adamts2\"]\n",
    "species = config.species\n",
    "\n",
    "\n",
    "def get_ensembl_id(header):\n",
    "    match = re.search(r\"gene:([^ ]+)\", header)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "dirname = \"/nemo/lab/znamenskiyp/scratch/spapros_10K_taylored_2021_16_ensembl/Adamts2/TempFolder20251027140913\"\n",
    "\n",
    "hits = retrieveseq.querygenes(genes, species)\n",
    "\n",
    "\"\"\"Load formatted RefSeq header and sequence files\"\"\"\n",
    "global Acronyms\n",
    "global Headers\n",
    "global Seq\n",
    "global Gencode\n",
    "\n",
    "try:\n",
    "    fastadir = os.path.join(config.BASE_DIR, config.reference_transcriptome)\n",
    "    if getattr(config, \"reference_transcriptome\", \"\").lower() == \"refseq\":\n",
    "        # Expect config.fasta_pre_suffix_refseq = (prefix, suffix), config.fasta_filenum_refseq = int\n",
    "        pre_suf = getattr(config, \"fasta_pre_suffix_refseq\", None)\n",
    "        filenum = getattr(config, \"fasta_filenum_refseq\", None)\n",
    "        acr_path = os.path.join(fastadir, species + \".acronymheaders.txt\")\n",
    "        if not os.path.isfile(acr_path):\n",
    "            print(\"Processing fasta database files..\")\n",
    "            if (\n",
    "                isinstance(pre_suf, (list, tuple))\n",
    "                and len(pre_suf) == 2\n",
    "                and isinstance(filenum, int)\n",
    "            ):\n",
    "                fastadb(\n",
    "                    fastadir,\n",
    "                    (pre_suf[0], pre_suf[1], filenum),\n",
    "                    species,\n",
    "                    source=\"refseq\",\n",
    "                    keep_nm_nr_only=True,\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Invalid RefSeq config: expected fasta_pre_suffix_refseq=(prefix, suffix) and fasta_filenum_refseq=int\"\n",
    "                )\n",
    "\n",
    "    elif getattr(config, \"reference_transcriptome\", \"\").lower() == \"ensembl\":\n",
    "        files = [config.cdna_file]\n",
    "        if hasattr(config, \"extra_files\") and config.extra_files:\n",
    "            files += config.extra_files\n",
    "        acr_path = os.path.join(fastadir, species + \".acronymheaders.txt\")\n",
    "        if not os.path.isfile(acr_path):\n",
    "            print(\"Processing fasta database files..\")\n",
    "            fastadb(\n",
    "                fastadir,\n",
    "                files,\n",
    "                species,\n",
    "                source=\"ensembl\",\n",
    "                keep_nm_nr_only=False,\n",
    "            )\n",
    "\n",
    "    # load database files\n",
    "    with open(os.path.join(fastadir, species + \".acronymheaders.txt\"), \"r\") as f:\n",
    "        Acronyms = [line.rstrip(\"\\n\") for line in f]\n",
    "    with open(os.path.join(fastadir, species + \".selectedheaders.txt\"), \"r\") as f:\n",
    "        Headers = [line.rstrip(\"\\n\") for line in f]\n",
    "    with open(os.path.join(fastadir, species + \".selectedseqs.txt\"), \"r\") as f:\n",
    "        Seq = [line.rstrip(\"\\n\") for line in f]\n",
    "    Gencode = retrieveseq.load_gencode()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Cannot load fasta database due to mismatch in species.\")\n",
    "\n",
    "\n",
    "def findseq(genes, hits, dirname):\n",
    "    \"\"\"Find target sequences, if multiple variants are found, call MSA\"\"\"\n",
    "    global Acronyms\n",
    "    global Headers\n",
    "    global Seq\n",
    "\n",
    "    targets = []\n",
    "    headers = []\n",
    "    basepos = []\n",
    "    msa = []\n",
    "    nocommon = []\n",
    "    variants = []\n",
    "\n",
    "    headersMSA = []\n",
    "\n",
    "    if config.reference_transcriptome == \"ensembl\":\n",
    "        hits, hitmask = hits[0], hits[1]\n",
    "\n",
    "    for c, hit in enumerate(hits):\n",
    "        if len(hit) == 1:  # only one variant\n",
    "            targets.append(Seq[hit[0]])\n",
    "            headers.append(Headers[hit[0]])\n",
    "            basepos.append([0, len(Seq[hit[0]]) - 1])\n",
    "            variants.append(Headers[hit[0]][1:].split(\".\", 1)[0])\n",
    "            file = (\n",
    "                dirname + \"/\" + genes[c] + \"_variants.fasta\"\n",
    "            )  # makes it easier to map properly the target if all sets of variants are written down\n",
    "            with open(file, \"w\") as f:\n",
    "                f.write(\"%s\\n\" % Headers[0])\n",
    "                f.write(\"%s\\n\\n\" % Seq[0])\n",
    "        else:  # more than ona variant\n",
    "            msa.append(genes[c])\n",
    "            tempheader = []\n",
    "            file = dirname + \"/\" + genes[c] + \"_variants.fasta\"\n",
    "            with open(file, \"w\") as f:\n",
    "                for multi in hit:\n",
    "                    f.write(\"%s\\n\" % Headers[multi])\n",
    "                    f.write(\"%s\\n\\n\" % Seq[multi])\n",
    "            if (\n",
    "                config.reference_transcriptome == \"ensembl\"\n",
    "            ):  # only keep variants that pass the GENCODE mask\n",
    "                hit = [h for i, h in enumerate(hit) if hitmask[i]]\n",
    "                if len(hit) == 1:\n",
    "                    msa = []  # we CLUSTAL as a function of the filtered variants\n",
    "                    targets.append(Seq[hit[0]])\n",
    "                    headers.append(Headers[hit[0]])\n",
    "                    basepos.append([0, len(Seq[hit[0]]) - 1])\n",
    "                file = dirname + \"/\" + genes[c] + \"_allowed_variants.fasta\"\n",
    "                print(f\"Writing allowed variants to {file}\")\n",
    "                with open(file, \"w\") as f:\n",
    "                    for multi in hit:\n",
    "                        f.write(\"%s\\n\" % Headers[multi])\n",
    "                        print(Headers[multi])\n",
    "                        f.write(\"%s\\n\\n\" % Seq[multi])\n",
    "                        tempheader.append(Headers[multi])\n",
    "                headersMSA.append(tempheader)\n",
    "                variants.append([i[1:].split(\".\", 1)[0] for i in tempheader])\n",
    "            else:  # Clustal on all variants, no filter\n",
    "                for multi in hit:\n",
    "                    tempheader.append(Headers[multi])\n",
    "                headersMSA.append(tempheader)\n",
    "                variants.append([i[1:].split(\".\", 1)[0] for i in tempheader])\n",
    "\n",
    "    # run multiple sequence alignment if more than one variant is found for any gene\n",
    "    if len(msa):\n",
    "        # process each gene independently\n",
    "        for gi, gene in enumerate(msa):\n",
    "            # headers for all variants of this gene (written earlier)\n",
    "            tempheader = headersMSA[gi]\n",
    "\n",
    "            round_no = 1  # first run uses *_variants.fasta / .aln / .dnd\n",
    "\n",
    "            def fasta_for_round(r: int) -> str:\n",
    "                if config.reference_transcriptome == \"ensembl\":\n",
    "                    return (\n",
    "                        f\"{dirname}/{gene}_allowed_variants.fasta\"\n",
    "                        if r == 1\n",
    "                        else f\"{dirname}/{gene}_variants_round{r}.fasta\"\n",
    "                    )\n",
    "                return (\n",
    "                    f\"{dirname}/{gene}_variants.fasta\"\n",
    "                    if r == 1\n",
    "                    else f\"{dirname}/{gene}_variants_round{r}.fasta\"\n",
    "                )\n",
    "\n",
    "            def dnd_for_round(r: int) -> str:\n",
    "                return (\n",
    "                    f\"{dirname}/{gene}_variants.dnd\"\n",
    "                    if r == 1\n",
    "                    else f\"{dirname}/{gene}_variants_round{r}.dnd\"\n",
    "                )\n",
    "\n",
    "            # how many variants do we currently have?\n",
    "            n_left = sum(1 for _ in SeqIO.parse(fasta_for_round(round_no), \"fasta\"))\n",
    "\n",
    "            while True:\n",
    "                # run MSA for this gene only (pass [gene] so continuemsa handles one job)\n",
    "                out = parmsa.continuemsa(\n",
    "                    dirname,\n",
    "                    [gene],\n",
    "                    round=None if round_no == 1 else round_no,\n",
    "                    reset=True,\n",
    "                )\n",
    "                # out = (Names, BasePos, Seqs); each is length 1 because we passed [gene]\n",
    "                name_c, basepos_c, seqs_c = out[0][0], out[1][0], out[2][0]\n",
    "\n",
    "                if len(basepos_c):  # consensus found\n",
    "                    # pick the header that matches the first sequence used by ClustalW\n",
    "                    # (the code relies on matching by substring of the chosen name)\n",
    "                    chosen = [h for h in tempheader if name_c in h]\n",
    "                    if not chosen:\n",
    "                        # fallback: just take the first header if matching fails\n",
    "                        chosen = [tempheader[0]]\n",
    "                    headers.append(chosen[0])\n",
    "                    basepos.append(basepos_c)\n",
    "                    targets.append(seqs_c)\n",
    "                    break\n",
    "\n",
    "                # no consensus: stop if ≤ 2 variants remain\n",
    "                if n_left <= 2:\n",
    "                    nocommon.append(gene)\n",
    "                    break\n",
    "\n",
    "                # drop one outgroup and try again\n",
    "                print(\n",
    "                    f\"[{gene}] No consensus, removing outgroup (round {round_no} → {round_no+1})\"\n",
    "                )\n",
    "                treefile = dnd_for_round(round_no)\n",
    "                outgroup = parmsa.find_outgroup(treefile)\n",
    "\n",
    "                in_fa = fasta_for_round(round_no)\n",
    "                round_no += 1\n",
    "                parmsa.remove_outgroup(in_fa, outgroup, round=round_no)\n",
    "\n",
    "                # update remaining variants\n",
    "                n_left = sum(1 for _ in SeqIO.parse(fasta_for_round(round_no), \"fasta\"))\n",
    "\n",
    "        print(\"MSA finished across genes.\")\n",
    "\n",
    "    return headers, basepos, targets, msa, nocommon, variants\n",
    "\n",
    "\n",
    "headers, basepos, targets, msa, nocommon, variants = findseq(genes, hits, dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e352e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
